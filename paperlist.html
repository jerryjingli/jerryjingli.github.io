<!DOCTYPE html>
<html>
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Jing Li - Publications</title>

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=0.85">

 
  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">


 <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src="js/jquery.min.js"></script>

  
    <!-- 添加自动计数器脚本 -->
<script src="js/auto-counter.js"></script>

  <script src="js/paper-id-generator.js"></script>


  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

       
 <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" type="text/css" href="css/custom.css"> 

        
       
    

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

   

   

  <script src="js/site.js"></script>




<!-- back to top button jquery -->
<!--     <script src="http://code.jquery.com/jquery-1.11.3.min.js"></script>    -->


  <!-- create the back to top button -->
    <script type="text/javascript">

    var amountScrolled = 300;

    $(window).scroll(function() {
      if ( $(window).scrollTop() > amountScrolled ) {
        $('a.back-to-top').fadeIn('slow');
      } else {
        $('a.back-to-top').fadeOut('slow');
      }
    });

    $('a.back-to-top, a.simple-back-to-top').click(function() {
      $('html, body').animate({
        scrollTop: 0
      }, 50);
      return false;
    });
    </script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110819220-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-110819220-2');
</script> -->

</head>



<body style="background-image: url('images/bg.png');">
<!-- <body> -->
     <!-- back to top button start -->
        <a href="" class="back-to-top">Back to Top</a>
        <!-- back to top button end -->




 <div class="container">



    <div  class="navbar-spacer"></div>
        <div style="background-image: url('images/bg.png');" class="navbar" > 
        <div class="container">  
            <ul class="navbar-list">
          <li class="navbar-item-left"><a class="navbar-link-left">Jing Li (<font face="华文行楷">李晶</font>)</a></li>
                   <li class="navbar-item"><a class="navbar-link" href="https://homepage.hit.edu.cn/lijinghit" target="_blank">中文</a></li>
                   <li class="navbar-item"><a class="navbar-link" href="team.html">Team</a></li>
                     <li class="navbar-item"><a class="navbar-link active" href="paperlist.html">Publications</a></li>
                <li class="navbar-item"><a class="navbar-link" href="index.html">Home</a></li>
             </ul>  
         </div>
         </div>










<div class="sidenav" style="font-size:90%;">

 <img align="middle"  class="headshot" id='JingLI80.jpg' src='images/JingLi19-1-100k.PNG' width='100%'/>

<!-- <div class="member">
      <div class="image">
              <img align="middle"  class="headshot" id='JingLI80.jpg' src='images/JingLi19.PNG' width='100%'>
                  <div class="hover-image">
                  <img align="middle"  class="headshot" id='jingli-hover.jpg' src='images/jingli-hover.jpg' width='100%' >
                </div>
      </div>
  </div> -->


                    <p style="font-size:120%;"> <b>Dr. Jing Li</b></p>
                <p> <b>Professor</b>, Harbin Institute of Technology (Shenzhen), China.</p>
                    <p><b>Email</b>: li.jing [AT] hit [DOT] edu [DOT] cn</p>



      <br/>


</div>






<div class="mymain">











<div class="docs-section">


<!-- <confol style="counter-reset: my-c-counter 9;">
<ol style="counter-reset: my-j-counter 20;">
<li>Coffee</li>
<li>Tea</li>
<confli>Coffee</confli>
<li>Milk</li>
<confli>Coffee</confli>
<confli>Coffee</confli>
</ol>
</confol>
 -->


      <h5><strong>Full List 
      [<a style="text-decoration:none;" href="https://scholar.google.com.sg/citations?hl=en&user=2QxEwWsAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a>] </strong></h5> 


<div id="book"> 
  <span class="sb"><b>Books</b></span> 
  <div class="wrapper"><a href="https://item.jd.com/10020053585372.html" target="_blank"><img src="images/KG2020-100k.png" width=150px align="left" /></a>
  <span >
     &nbsp &nbsp Authors: Xiaoyan Zhu, <strong>Jing Li</strong>, Yu Hao, Han Xiao and Minlie Huang<br>
    &nbsp &nbsp Publisher: Publishing House of Electronics Industry <br>
    &nbsp &nbsp ISBN: 9787121389924 <br>
    &nbsp &nbsp Publish time: June 1st, 2020<br>
       &nbsp &nbsp Links: <a style="text-decoration:none;" href="https://item.jd.com/10020053585372.html" target="_blank">JD.com</a>, <a style="text-decoration:none;" href="https://detail.tmall.com/item.htm?abbucket=10&detail_redpacket_pop=true&id=721290455247&ltk2=1746108474303vbeinjejqk9vwzj5yh62i&ns=1&priceTId=2150463617461082346711363e1d45&query=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF&skuId=5833117246289&spm=a21n57.1.hoverItem.18&utparam=%7B%22aplus_abtest%22%3A%22a978ad9532d4e1911450c3528f87e561%22%7D&xxc=taobaoSearch" target="_blank">Tmall.com</a><br>
    <br>
<br>
<br>
      </span></div>
</div>

      

  <jol>
 <confol>


  <!-- <p style="margin: 0rem 0 1rem -3rem;"><strong>Submitted</strong></p>  --> <!-- style="margin: 0rem 0 1rem -4rem;" -->




<!-- <h3 class="yearsubmitted">Submitted</h3>

<li > <strong> Named Entity Boundary Detection via Meta-Learning</strong><br/>
       <i> <strong>Jing Li</strong><br/>
<strong>Under review</strong>
</i> </li>


<li > <strong> Few-Shot Named Entity Recognition</strong><br/>
       <i> <strong>Jing Li</strong><br/>
<strong>Under review</strong>
</i> </li>

 -->


<!-- <h3 class="year">2020</h3>
 -->




<dl>
  <dt><b>Preprints</b></dt>
  

  <dd><strong>FLM-101B: An Open LLM and How to Train It with $100K Budget </strong></dd> 

   <dd><i> Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, <strong>Jing Li</strong>, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun and Yequan Wang </i></dd> 

     <dd style="margin-bottom:3mm">
     <a class="file_link" href="https://arxiv.org/pdf/2309.03852.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

     <a class="file_link" href="https://huggingface.co/CofeAI/FLM-101B"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_101b').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_101b').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 



      <div id="abstract_101b" class="verbatim" style="display:none">
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model FLM-101B, trained with a budget of $100K, achieves comparable performance to powerful and well-known models, eg GPT-3 and GLM-130B, especially in the IQ benchmark evaluations with contexts unseen in training data. The checkpoint of FLM-101B will be open-sourced at https://huggingface.co/CofeAI/FLM-101B.
   </div>

      <div id="bibtex_101b" style="display:none">
        <pre class="verbatim">
@article{DBLP:journals/corr/abs-2309-03852,
  author       = {Xiang Li and
                  Yiqun Yao and
                  Xin Jiang and
                  Xuezhi Fang and
                  Xuying Meng and
                  Siqi Fan and
                  Peng Han and
                  Jing Li and
                  Li Du and
                  Bowen Qin and
                  Zheng Zhang and
                  Aixin Sun and
                  Yequan Wang},
  title        = {{FLM-101B:} An Open {LLM} and How to Train It with {\textdollar}100K
                  Budget},
  journal      = {CoRR},
  volume       = {abs/2309.03852},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.03852},
  doi          = {10.48550/arXiv.2309.03852},
  eprinttype   = {arXiv},
}
      </pre>
      </div>

    </dd> 


<!-- 
  <dd><strong>Multi-objective Large Language Model Alignment with Hierarchical Experts </strong></dd> 

   <dd><i> Zhuo Li, Guodong Du, WeiYang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> </i></dd> 

     <dd style="margin-bottom:3mm">
     <a class="file_link" href="https://arxiv.org/pdf/2505.20925"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

     <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_hoe').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_hoe').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 



      <div id="abstract_hoe" class="verbatim" style="display:none">
Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce HoE (Hierarchical Mixture-of-Experts), a lightweight, parameter-efficient, and plug-and-play approach that eliminates the need for model retraining, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, HoE consists of three hierarchical components: LoRA Experts, Router Experts and Weighting Router, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate HoE across various tasks on 16 objectives and 200 different preferences among 8 benchmarks, demonstrating superior performance over 15 recent baselines.
   </div>

      <div id="bibtex_hoe" style="display:none">
        <pre class="verbatim">
@article{hoe2025,
  title        = {Multi-objective Large Language Model Alignment with Hierarchical Experts},
  author       = {Zhuo Li and Guodong Du and WeiYang Guo and Yigeng Zhou and Xiucheng Li and Wenya Wang and Fangming Liu and Yequan Wang and Deheng Ye and Min Zhang and Jing Li},
  journal      = {CoRR},
  volume       = {abs/2505.20925},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2505.20925},
  doi          = {10.48550/arXiv.2309.03852},
  eprinttype   = {arXiv},
}
      </pre>
      </div>

    </dd> 

-->


  <dd><strong>Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning </strong></dd> 

   <dd><i> Weiyang Guo, Zesheng Shi, Zhuo Li, Yequan Wang, Xuebo Liu, Wenya Wang, Fangming Liu, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> </i></dd> 

     <dd style="margin-bottom:3mm">
     <a class="file_link" href="https://arxiv.org/pdf/2506.00782"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

     <a class="file_link" href="https://github.com/yuki-younai/Jailbreak-R1"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_jailr1').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_jailr1').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 



      <div id="abstract_jailr1" class="verbatim" style="display:none">
As large language models (LLMs) grow in power and influence, ensuring their safety and preventing harmful output becomes critical. Automated red teaming serves as a tool to detect security vulnerabilities in LLMs without manual labor. However, most existing methods struggle to balance the effectiveness and diversity of red-team generated attack prompts. To address this challenge, we propose Jailbreak-R1, a novel automated red teaming training framework that utilizes reinforcement learning to explore and generate more effective attack prompts while balancing their diversity. Specifically, it consists of three training stages: (1) Cold Start: The red team model is supervised and fine-tuned on a jailbreak dataset obtained through imitation learning. (2) Warm-up Exploration: The model is trained in jailbreak instruction following and exploration, using diversity and consistency as reward signals. (3) Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually enhance the jailbreak performance of the red-team model. Extensive experiments on a variety of LLMs show that Jailbreak-R1 effectively balances the diversity and effectiveness of jailbreak prompts compared to existing methods. Our work significantly improves the efficiency of red team exploration and provides a new perspective on automated red teaming. Code is available at https://github.com/yuki-younai/Jailbreak-R1.
   </div>

      <div id="bibtex_jailr1" style="display:none">
        <pre class="verbatim">
@article{jailbreakr12025,
  title        = {Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning},
  author       = {Weiyang Guo, Zesheng Shi, Zhuo Li, Yequan Wang, Xuebo Liu, Wenya Wang, Fangming Liu, Min Zhang, Jing Li},
  journal      = {CoRR},
  volume       = {abs/2505.20925},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2505.20925},
  doi          = {10.48550/arXiv.2309.03852},
  eprinttype   = {arXiv},
}
      </pre>
      </div>

    </dd> 



<!-- 

  <dd><strong>Knowledge Grafting of Large Language Models</strong></dd> 

   <dd><i>Guodong Du, Xuanning Zhou, Junlin Li, Zhuo Li, Zesheng Shi, Wanyu Lin <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> </i></dd> 

     <dd>
     <a class="file_link" href="https://arxiv.org/pdf/2505.18502"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

     <a class="file_link" href="https://github.com/duguodong7/GraftLLM"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_crafting').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_crafting').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <div id="abstract_crafting" class="verbatim" style="display:none">
Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: https://github.com/duguodong7/GraftLLM.
   </div>

      <div id="bibtex_crafting" style="display:none">
        <pre class="verbatim">
@article{crafting2025,
  title        = {Knowledge Grafting of Large Language Models},
  author       = {Guodong DU and Xuanning Zhou and Junlin Li and Zhuo Li and Zesheng Shi and Wanyu Lin and Ho-Kin Tang and Xiucheng Li and Fangming Liu and Wenya Wang and Min Zhang and Jing Li},
  journal      = {CoRR},
  volume       = {abs/2505.18502},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2505.18502},
  doi          = {10.48550/arXiv.2505.18502},
  eprinttype   = {arXiv},
}
      </pre>
      </div>

    </dd> 

-->



<!-- <br> -->


</dl>





<!-- <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> -->



<!-- <confli > <strong>template for paper</strong><br/>
      <i>Yifan Lu<br/>
          <span class="abbpaper">ACL-25</span>- ACL 25 <br/>
          <a class="file_link" href="https://arxiv.org/abs/2412.13782"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/ABi-dot/Kedkg"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
         abstract
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
bibtex
          </pre>
          </div>
    
    </i> </confli> -->




<h3 class="year">2026</h3>







<confli > <strong>Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning</strong><br/>
        <i>Zhuoyuan Hao, Zhuo Li, Wu Li, Fangming Liu, Min Zhang and <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> <br/>
      <span class="abbpaper">ICLR-26</span>- The Fourteenth International Conference on Learning Representations, 2026. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2602.06600"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/hhh2210/echoes-as-anchors"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic thinking tokens and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain---and often ignore---the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} 
 as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate under identical decoding settings and compute budgets on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines.
</div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{echoes_iclr26,
  title={Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning},
  author = {Zhuoyuan Hao and Zhuo Li and Wu Li and Fangming Liu and Min Zhang and Jing Li}
  booktitle = {The Fourteenth International Conference on Learning Representations (ICLR)},
  year={2026}
}
          </pre>
          </div>
    
    </i> </confli>







<confli > <strong>Multi-objective Large Language Model Alignment with Hierarchical Experts</strong><br/>
        <i>Zhuo Li, Guodong Du, WeiYang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> <br/>
      <span class="abbpaper">ICLR-26</span>- The Fourteenth International Conference on Learning Representations, 2026. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2505.20925"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce HoE (Hierarchical Mixture-of-Experts), a lightweight, parameter-efficient, and plug-and-play approach that eliminates the need for model retraining, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, HoE consists of three hierarchical components: LoRA Experts, Router Experts and Weighting Router, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate HoE across various tasks on 16 objectives and 200 different preferences among 8 benchmarks, demonstrating superior performance over 15 recent baselines. </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{hoe_iclr26,
  title={Multi-objective Large Language Model Alignment with Hierarchical Experts},
  author = {Zhuo Li and Guodong Du and WeiYang Guo and Yigeng Zhou and Xiucheng Li and Wenya Wang and Fangming Liu and Yequan Wang and Deheng Ye and Min Zhang and Jing Li},
  booktitle = {The Fourteenth International Conference on Learning Representations (ICLR)},
  year={2026}
}
          </pre>
          </div>
    
    </i> </confli>




<confli > <strong>Knowledge Fusion of Large Language Models via Modular SkillPacks</strong><br/>
        <i>Guodong DU, Zhuo Li, Xuanning Zhou, Junlin Li, Zesheng Shi, Wanyu Lin, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup><br/>
      <span class="abbpaper">ICLR-26</span>- The Fourteenth International Conference on Learning Representations, 2026. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2505.18502"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/duguodong7/GraftLLM"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Cross-capability transfer represents a key challenge in large language model (LLM) research, particularly in multi-task integration, model compression, and knowledge fusion. Recent works such as FuseLLM and FuseChat have shown the potential of transferring multiple model capabilities to lightweight models, thereby enhancing adaptability and efficiency. This motivates our investigation into more efficient methods for cross-capability transfer. However, existing merging approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model’s inherent capability and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel grafting-based method that stores source model capabilities in a target model + SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy for parameter updates, ensuring efficient storage while preserving task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous LLM fusion. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer.  </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{guodong_iclr26,
  title={Knowledge Fusion of Large Language Models via Modular SkillPacks},
  author = {Guodong DU and Zhuo Li and Xuanning Zhou and Junlin Li and Zesheng Shi and Wanyu Lin and Ho-Kin Tang and Xiucheng Li and Fangming Liu and Wenya Wang and Min Zhang and Jing Li},
  booktitle = {The Fourteenth International Conference on Learning Representations (ICLR)},
  year={2026}
}
          </pre>
          </div>
    
    </i> </confli>






<li > <strong>Mitigating Context-Memory Conflicts in LLMs through Dynamic Cognitive Reconciliation Decoding</strong><br/>
        <i>Yigeng Zhou, Wu Li, Yifan Lu, Yequan Wang, Xuebo Liu, Wenya Wang, Jun Yu, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup><br/>
      <span class="abbpaper">IEEE TASL</span>- IEEE Transactions on Audio, Speech and Language Processing, 2026<br/>
          <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Large language models accumulate extensive parametric knowledge through pre-training. However, knowledge conflicts occur when outdated or incorrect parametric knowledge conflicts with external knowledge in the context. Existing methods address knowledge conflicts through contrastive decoding, but in conflict-free scenarios, static approaches disrupt output distribution. Other dynamic decoding methods attempt to measure the degree of conflict but still struggle with complex real-world situations. In this paper, we propose a two-stage decoding method called Dynamic Cognitive Reconciliation Decoding (DCRD), to predict and mitigate context-memory conflicts. DCRD first analyzes the attention map to assess context fidelity and predict potential conflicts. Based on this prediction, the input is directed to one of two decoding paths: (1) greedy decoding, or (2) context
fidelity-based dynamic decoding. This design enables DCRD to handle conflicts efficiently while maintaining high accuracy and decoding efficiency in conflict-free cases. Additionally, to simulate scenarios with frequent knowledge updates, we constructed ConflictKG, a knowledge conflict QA benchmark. Experiments on four LLMs across six QA datasets show that DCRD outperforms all baselines, achieving state-of-the-art performance. </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@article{dcrd_26,
  title={Mitigating Context-Memory Conflicts in LLMs through Dynamic Cognitive Reconciliation Decoding},
  author = {Yigeng Zhou and Wu Li and Yifan Lu and Yequan Wang and Xuebo Liu and Wenya Wang and Jun Yu and Min Zhang and Jing Li},
  booktitle = {IEEE Transactions on Audio, Speech and Language Processing},
  year={2026}
}
          </pre>
          </div>
    
    </i> </li>





<li > <strong>Progressive Adaptation of Large Language Models for Multilingual Text Ranking</strong><br/>
        <i>Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, <strong>Jing Li</strong>, Min Zhang<br/>
      <span class="abbpaper">ACM TOIS-26</span>- ACM Transactions on Information Systems, 2026. <br/>
          <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Despite increasing research attention to text ranking, most studies focus on monolingual scenarios, with a particular emphasis on
English-language contexts. This narrow focus limits the applicability of ranking models in cross-lingual contexts, such as ranking
Chinese documents based on English queries. Recent advances in large language models (LLMs) have significantly reduced interlanguage barriers through extensive pre-training on extensive multilingual corpora, thus facilitating the study of multilingual text
ranking (MTR). In this work, we explore the potential of LLMs in MTR tasks. Specifically, we first introduce an MTR benchmark
encompassing both monolingual and cross-lingual scenarios. Then, we propose a two-stage training pipeline to alleviate the misalignment between LLMs and text ranking. Lastly, we adapt this training pipeline to multilingual scenarios from the perspective of
training data and methods. Our experiments on the MTR benchmark demonstrate that the proposed multilingual two-stage training
pipeline significantly improves LLM ranking performance in both monolingual and cross-lingual scenarios, particularly in out-domain
settings. We complement these findings with a thorough analysis to deepen the understanding of our approach.  </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@article{mtr_26,
  title={Progressive Adaptation of Large Language Models for Multilingual Text Ranking},
  author = {Longhui Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang and Jing Li and Min Zhang},
  booktitle = {ACM Transactions on Information Systems},
  year={2026}
}
          </pre>
          </div>
    
    </i> </li>





<li > <strong>Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models</strong><br/>
        <i>Jun Rao, Xuebo Liu, Zepeng Lin, Liang Ding, <strong>Jing Li</strong>, Min Zhang<br/>
      <span class="abbpaper">ELSEVIER KBS-26</span>- ELSEVIER Knowledge-Based Systems, 2026. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2409.12512"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student-generated output (SGO) to combat exposure bias. Our theoretical analyses and experimental validation reveal that while Reverse KL effectively mimics certain features of the teacher distribution, it fails to capture most of its behaviors. Conversely, SGO incurs higher computational costs and presents challenges in optimization, particularly when the student model is significantly smaller than the teacher model. These constraints are primarily due to the immutable distribution of the teacher model, which fails to adjust adaptively to models of varying sizes. We introduce Online Knowledge Distillation (OKD), where the teacher network integrates online modules to concurrently train with the student model. This strategy abolishes the necessity for on-policy sampling and merely requires minimal updates to the parameters of the teacher’s online module during training, thereby allowing dynamic adaptation to the student’s distribution to make distillation better. Extensive results across multiple generation datasets show that OKD achieves or exceeds the performance of leading methods across diverse model sizes within autoregressive families, reducing training time by up to fourfold. </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@article{okd_26,
  title={Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models},
  author = {Jun Rao and Xuebo Liu and Zepeng Lin and Liang Ding and Jing Li and Min Zhang},
  booktitle = {Knowledge-Based Systems (KBS)},
  year={2026}
}
          </pre>
          </div>
    
    </i> </li>






<confli > <strong>FP=XINT: Representing Neural Networks via Low‑Bit Series Basis Functions</strong><br/>
        <i>Boyang Zhang, Daning Cheng, Yunquan Zhang, Jiake Tian, <strong>Jing Li</strong>, Fangming Liu<br/>
      <span class="abbpaper">AAAI-26</span>- The Fortieth AAAI Conference on Artificial Intelligence, 2026. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2412.06865v2"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Deep neural networks are often over-parameterized, resulting in prohibitive storage and computational costs. A fundamental question is whether a complex network can be re-expressed in terms of a compact set of basis functions without sacrificing accuracy. Motivated by this perspective, we aim to approximate a dense model by decomposing it into a small number of lightweight components that capture the essential functional structure of the network. To this end, we propose a series expansion framework that rewrites a neural network as a linear combination of low-bit basis models. Within the post-training quantization setting, the full-precision model is expanded hierarchically at the tensor, layer, and model levels into a structured set of basis functions. We theoretically prove that this expansion converges exponentially to the original model. Furthermore, we design AbelianAdd and AbelianMul operations between isomorphic basis models, endowing the expansion with an Abelian group structure that naturally supports commutative and parallel computation. Experimental results across diverse architectures show that our series expansion method leverages a set of ultra-low-bit basis functions, not only preserving full-precision performance without the need for calibration data or fine-tuning, but also featuring a parallel-friendly design that enables efficient and scalable deployment.    </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{boyang_aaai26,
  title={FP=XINT: Representing Neural Networks via Low‑Bit Series Basis Functions},
  author = {Boyang Zhang and Daning Cheng and Yunquan Zhang and Jiake Tian and Jing Li and Fangming Liu },
  booktitle = {The Fortieth AAAI Conference on Artificial Intelligence (AAAI)},
  year={2026}
}
          </pre>
          </div>
    
    </i> </confli>



<h3 class="year">2025</h3>






<confli > <strong>Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization</strong><br/>
        <i>Chenrui Wang, Junyi Shu, Billy Chiu, YU LI, Saleh Alharbi, Min Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup><br/>
      <span class="abbpaper">NeurIPS-25</span>- The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2510.15976v1"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/fattyray/learning-to-watermark/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
 The rapid development of LLMs has raised concerns about their potential misuse, leading to various watermarking schemes that typically offer high detectability. However, existing watermarking techniques often face trade-off between watermark detectability and generated text quality. In this paper, we introduce Learning to Watermark (LTW), a novel selective watermarking framework that leverages multi-objective optimization to effectively balance these competing goals. LTW features a lightweight network that adaptively decides when to apply the watermark by analyzing sentence embeddings, token entropy, and current watermarking ratio. Training of the network involves two specifically constructed loss functions that guide the model toward Pareto-optimal solutions, thereby harmonizing watermark detectability and text quality. By integrating LTW with two baseline watermarking methods, our experimental evaluations demonstrate that LTW significantly enhances text quality without compromising detectability. Our selective watermarking approach offers a new perspective for designing watermarks for LLMs and a way to preserve high text quality for watermarks.       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{chenrui_neruips25,
  title={Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization},
  author = {Chenrui Wang and Junyi Shu and Billy Chiu and YU LI and Saleh Alharbi and Min Zhang and Jing Li},
  booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>





<confli > <strong>NeurIPT: Foundation Model for Neural Interfaces</strong><br/>
        <i>Zitao Fang, CHENXUAN LI, Zhou Hongting, Shuyang Yu, Guodong DU, Ashwaq Qasem, Yang Lu, <strong>Jing Li</strong>, Junsong Zhang, Sim Kuan Goh <br/>
      <span class="abbpaper">NeurIPS-25</span>- The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2510.16548"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://zzzitaofang.github.io/projects/NeurIPT/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
 Electroencephalography (EEG) has wide-ranging applications, from clinical diagnosis to brain-computer interfaces (BCIs). With the increasing volume and variety of EEG data, there has been growing interest in establishing foundation models (FMs) to scale up and generalize neural decoding. Despite showing early potential, applying FMs to EEG remains challenging due to substantial inter-subject, inter-task, and inter-condition variability, as well as diverse electrode configurations across recording setups. To tackle these open challenges, we propose NeurIPT, a foundation model tailored for diverse EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP), masking based on signal amplitude rather than random intervals, to learn robust representations across varying signal intensities beyond local interpolation. Moreover, this temporal representation is enhanced by a progressive Mixture-of-Experts (MoE) architecture, where specialized expert subnetworks are progressively introduced at deeper layers, adapting effectively to the diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages the 3D physical coordinates of electrodes, enabling effective transfer across varying EEG settings, and develops Intra-Inter Lobe Pooling (IILP) during fine-tuning to efficiently exploit regional brain features. Empirical evaluations across nine downstream BCI datasets, via fine-tuning and training from scratch, demonstrated NeurIPT consistently achieved state-of-the-art performance, highlighting its broad applicability and robust generalization. Our work pushes forward the state of FMs in EEG and offers insights into scalable and generalizable neural information processing systems.   
</div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{zitao_neruips25,
  title={NeurIPT: Foundation Model for Neural Interfaces},
  author = {Zitao Fang and CHENXUAN LI and Zhou Hongting and Shuyang Yu and Guodong DU and Ashwaq Qasem and Yang Lu and Jing Li and Junsong Zhang and Sim Kuan Goh},
  booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>




<confli > <strong>Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning</strong><br/>
        <i>Yihong Tang, Kehai Chen, Muyun Yang, Zheng-Yu Niu, <strong>Jing Li</strong>, Tiejun Zhao, Min Zhang<br/>
      <span class="abbpaper">NeurIPS-25</span>- The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2506.01748"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
      The advancement of Large Language Models (LLMs) has spurred significant interest in Role-Playing Agents (RPAs) for applications such as emotional companionship and virtual interaction. However, recent RPAs are often built on explicit dialogue data, lacking deep, human-like internal thought processes, resulting in superficial knowledge and style expression. While Large Reasoning Models (LRMs) can be employed to simulate character thought, their direct application is hindered by attention diversion (i.e., RPAs forget their role) and style drift (i.e., overly formal and rigid reasoning rather than character-consistent reasoning). To address these challenges, this paper introduces a novel Role-Aware Reasoning (RAR) method, which consists of two important stages: Role Identity Activation (RIA) and Reasoning Style Optimization (RSO). RIA explicitly guides the model with character profiles during reasoning to counteract attention diversion, and then RSO aligns reasoning style with the character and scene via LRM distillation to mitigate style drift. Extensive experiments demonstrate that the proposed RAR significantly enhances the performance of RPAs by effectively addressing attention diversion and style drift.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{yihong_neruips25,
  title={Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning},
  author = {Yihong Tang and Kehai Chen and Muyun Yang and Zheng-Yu Niu and Jing Li and Tiejun Zhao and Min Zhang},
  booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025}
}
   
          </pre>
          </div>
    
    </i> </confli>





<confli > <strong>STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment</strong><br/>
        <i>Jiaqian Li, Qisheng Hu, <strong>Jing Li</strong>, Wenya Wang<br/>
      <span class="abbpaper">EMNLP-25</span>- The 2025 Conference on Empirical Methods in Natural Language Processing. <br/>
          <a class="file_link" href="https://aclanthology.org/2025.emnlp-main.746.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
   In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{jiaqian_emnlp25,
  title={STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment},
  author = {Jiaqian Li and Qisheng Hu and Jing Li and Wenya Wang},
  booktitle = {The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>


    


<confli > <strong>To See a World in a Spark of Neuron: Disentangling Multi-Task Interference for Training-Free Model Merging</strong><br/>
        <i>Zitao Fang, Guodong DU, Shuyang Yu, Yifei Guo, Yiwei Zhang, Yiyao Cao, <strong>Jing Li</strong>, Ho-Kin Tang, Sim Kuan Goh<br/>
      <span class="abbpaper">EMNLP-25</span>- The 2025 Conference on Empirical Methods in Natural Language Processing. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2503.05320"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
   Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlook the fundamental roles of neurons, their connectivity, and activation, resulting in a merging process and a merged model that does not consider how neurons relay and process information. In this work, we present the first study that relies on neuronal mechanisms for model merging. We decompose task-specific representations into two complementary neuronal subspaces that regulate neuron sensitivity and input adaptability. Leveraging this decomposition, we introduce NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrate that NeuroMerging achieves superior performance compared to existing methods on multi-task benchmarks across both natural language and vision domains. Our findings highlight the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion. Code will be released upon acceptance.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{zitao_emnlp25,
  title={To See a World in a Spark of Neuron: Disentangling Multi-Task Interference for Training-Free Model Merging},
  author = {Zitao Fang and Guodong DU and Shuyang Yu and Yifei Guo and Yiwei Zhang and Yiyao Cao and Jing Li and Ho-Kin Tang and Sim Kuan Goh},
  booktitle = {The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>




<confli > <strong>System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition</strong><br/>
        <i>Jiahao Wang, Ramen Liu, Longhui Zhang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup><br/>
      <span class="abbpaper">CCL-25-</span>- The 24th China National Conference on Computational Linguistics, 2025. (<strong style="color:#FF00FF">The Second Place</strong>)<br/>
          <a class="file_link" href="https://arxiv.org/pdf/2507.18580?"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/king-wang123/CCL25-SRAG-MAV"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
 This paper presents our system for CCL25-Eval Task 10, addressing Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and applies multi-round inference with voting to improve output stability and performance. Our system, based on the Qwen2.5-7B model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o (Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.    
</div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{jiahao_ccl25,
  title={System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition},
  author = {Jiahao Wang and Ramen Liu and Longhui Zhang and Jing Li},
  booktitle = {The 24th China National Conference on Computational Linguistics},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>





<confli > <strong>Function-to-Style Guidance of LLMs for Code Translation</strong><br/>

  <i>Longhui Zhang, Bin Wang, Jiahao Wang, Xiaofeng Zhao, Min Zhang, Hao Yang, Meishan Zhang, Yu Li, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Jun Yu, Min Zhang<br/>
      <span class="abbpaper">ICML-25</span>- The Forty-Second International Conference on Machine Learning, 2025. <br/>
      <a class="file_link" href="https://arxiv.org/pdf/2507.11083"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

      <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_F2STrans').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_F2STrans').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_F2STrans" class="verbatim" style="display:none">
    Large language models (LLMs) have made significant strides in code translation tasks. However, ensuring both the correctness and readability of translated code remains a challenge, limiting their effective adoption in real-world software development. In this work, we propose F2STrans, a function-to-style guiding paradigm designed to progressively improve the performance of LLMs in code translation. Our approach comprises two key stages: (1) Functional learning, which optimizes translation correctness using high-quality source-target code pairs mined from online programming platforms, and (2) Style learning, which improves translation readability by incorporating both positive and negative style examples. Additionally, we introduce a novel code translation benchmark that includes up-to-date source code, extensive test cases, and manually annotated ground-truth translations, enabling comprehensive functional and stylistic evaluations. Experiments on both our new benchmark and existing datasets demonstrate that our approach significantly improves code translation performance. Notably, our approach enables Qwen-1.5B to outperform prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code translation scenarios.
   </div>

      <div id="bibtex_F2STrans" style="display:none">
        <pre class="verbatim">
@inproceedings{F2STrans_icml25,
  title={Function-to-Style Guidance of LLMs for Code Translation},
  author = {Longhui Zhang and Bin Wang and Jiahao Wang and Xiaofeng Zhao and Min Zhang and Hao Yang and Meishan Zhang and Yu Li and Jing Li and Jun Yu and Min Zhang},
  booktitle = {The Forty-Second International Conference on Machine Learning (ICML)},
  year={2025}
}
      </pre>
      </div>

</i> </confli>






<confli > <strong>Few-Shot Learner Generalizes Across AI-Generated Image Detection</strong><br/>
      <i>Shiyu Wu, Jing Liu, <strong>Jing Li</strong>, Yequan Wang<br/>
          <span class="abbpaper">ICML-25</span>- The Forty-Second International Conference on Machine Learning, 2025. <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2501.08763"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/teheperinko541/Few-Shot-AIGI-Detector"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
        Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, these detectors suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space to effectively distinguish unseen fake images by utilizing very few samples. Experiments show that FSD achieves state-of-the-art performance by 
 average ACC on GenImage dataset. More importantly, our method is better capable of capturing the intra-category common features in unseen images without further training.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{fsd_icml25,
  title={Few-Shot Learner Generalizes Across AI-Generated Image Detection},
  author = {Shiyu Wu and Jing Liu and Jing Li and Yequan Wang},
  booktitle = {The Forty-Second International Conference on Machine Learning (ICML)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>






<confli > <strong>MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming</strong><br/>
      <i>WeiYang Guo, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Wenya Wang, Yu Li, Daojing He, Jun Yu, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- The 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2505.17147"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/yuki-younai/MTSA"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
         The proliferation of jailbreak attacks against large language models (LLMs) highlights the need for robust security measures. However, in multi-round dialogues, malicious intentions may be hidden in interactions, leading LLMs to be more prone to produce harmful responses. In this paper, we propose the Multi-Turn Safety Alignment (MTSA) framework, to address the challenge of securing LLMs in multi-round interactions. It consists of two stages: In the thought-guided attack learning stage, the red-team model learns about thought-guided multi-round jailbreak attacks to generate adversarial prompts. In the adversarial iterative optimization stage, the red-team model and the target model continuously improve their respective capabilities in interaction. Furthermore, we introduce a multi-turn reinforcement learning algorithm based on future rewards to enhance the robustness of safety alignment. Experimental results show that the red-team model exhibits state-of-the-art attack capabilities, while the target model significantly improves its performance on safety benchmarks.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{weiyang_acl25,
  title={MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming},
  author = {WeiYang Guo and Jing Li and Yu Li and Wenya Wang and Daojing He and Jun Yu and Min Zhang},
  booktitle = {The 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
    
          </pre>
          </div>
    
    </i> </confli>




<confli > <strong>Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling</strong><br/>
      <i>Junlin Li, Guodong DU, <strong>Jing Li</strong> <sup><i  style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Sim Kuan Goh, Wenya Wang, Yequan Wang, Fangming Liu, Ho-Kin Tang, Saleh Alharbi, Daojing He, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- The 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://www.arxiv.org/pdf/2505.17110"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
     Fine-tuning Large Language Models (LLMs) with multimodal encoders on modality-specific data expands the modalities that LLMs can handle, leading to the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies on resource-intensive and inflexible fine-tuning from scratch with new multimodal data. In this paper, we propose MMER (Multi-modality Expansion and Retention), a training-free approach that integrates existing MLLMs for effective multimodal expansion while retaining their original performance. Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM parameters. By comparing original and merged LLM parameters, MMER generates binary masks to approximately separate LLM parameters for each modality. These decoupled parameters can independently process modality-specific inputs, reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can also mitigate catastrophic forgetting by applying a similar process to MLLMs fine-tuned on new tasks. Extensive experiments show significant improvements over baselines, proving that MMER effectively expands LLMs' multimodal capabilities while retaining 99% of the original performance, and also markedly mitigates catastrophic forgetting.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{junlin_acl25,
  title={Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling},
  author = {Junlin Li and Guodong DU and Jing Li  and Sim Kuan Goh and Wenya Wang and Yequan Wang and Fangming Liu and Ho-Kin Tang and Saleh Alharbi and Daojing He and Min Zhang},
  booktitle = {The 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
    
          </pre>
          </div>
    
    </i> </confli>





<confli > <strong>Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer</strong><br/>
      <i>Guodong DU, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Zitao Fang, Junlin Li, Runhua Jiang, Shuyang Yu, Yifei Guo, Yangneng Chen, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Honghai LIU, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- The 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2505.18713"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/duguodong7/NPS-Pruning"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
    Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called Neural Parameter Search (NPS) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{guodong_acl25,
  title={Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer},
  author = {Guodong DU and Jing Li and Zitao Fang and Junlin Li and Runhua Jiang and Shuyang Yu and Yifei Guo and Yangneng Chen and Sim Kuan Goh and Ho-Kin Tang and Daojing He and Honghai LIU and Min Zhang},
  booktitle = {The 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
    
          </pre>
          </div>
    
    </i> </confli>





<confli > <strong>Safety Alignment via Constrained Knowledge Unlearning</strong><br/>
      <i>Zesheng Shi, Yucheng Zhou, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Yuxin Jin, YU LI, Daojing He, Fangming Liu, Saleh Alharbi, Jun Yu, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- The 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2505.18588"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Despite significant progress in safety alignment, large language models (LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms have not fully deleted harmful knowledge in LLMs, which allows such attacks to bypass safeguards and produce harmful outputs. To address this challenge, we propose a novel safety alignment strategy, Constrained Knowledge Unlearning (CKU), which focuses on two primary objectives: knowledge localization and retention, and unlearning harmful knowledge. CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge. During the unlearning process, CKU prunes the gradients of neurons in U to preserve valuable knowledge while effectively mitigating harmful content. Experimental results demonstrate that CKU significantly enhances model safety without compromising overall performance, offering a superior balance between safety and utility compared to existing methods. Additionally, our analysis of neuron knowledge sensitivity across various MLP layers provides valuable insights into the mechanics of safety alignment and model knowledge editing.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{zesheng_acl25,
  title={Safety Alignment via Constrained Knowledge Unlearning},
  author = {Zesheng Shi and Yucheng Zhou and Jing Li and Yuxin Jin and YU LI and Daojing He and Fangming Liu and Saleh Alharbi and Jun Yu and Min Zhang},
  booktitle = {The 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
    
          </pre>
          </div>
    
    </i> </confli>








<confli > <strong>Speed Up Your Code: Progressive Code Acceleration Through Bidirectional Tree Editing</strong><br/>
      <i>Longhui Zhang, Jiahao Wang, Meishan Zhang, GaoXiong Cao, Ensheng Shi, mayuchi, Jun Yu, Honghai LIU, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- The 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://aclanthology.org/2025.acl-long.1387.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Large language models (LLMs) have made significant strides in code acceleration (CA) tasks. Current works typically fine-tune LLMs using slow-fast code pairs mined from online programming platforms. Although these methods are widely recognized for their effectiveness, the training data often lack clear code acceleration patterns and offer only limited speed improvements. Moreover, existing training methods, such as direct instruction fine-tuning (IFT), tend to overlook the hierarchical relationships among acceleration patterns. In this work, we introduce BITE, a novel training paradigm designed to improve LLMs' CA capabilities through two key innovations: (1) Bidirectional tree editing, which generates high-quality training data by incrementally transforming given code into both its most efficient and least efficient variants, and (2) Progressive code acceleration learning, which enables LLMs to internalize multi-level CA strategies by learning increasingly sophisticated acceleration patterns. Additionally, we introduce a new CA evaluation benchmark and metric for comprehensive assessment of model performance on CA tasks. Extensive experiments on both our benchmark and existing benchmarks demonstrate the effectiveness of our approach. Notably, BITE enables Qwen-1.5B to outperform prompt-enhanced GPT-4 and current training-based methods on average across five programming languages.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{longhui_acl25,
  title={Speed Up Your Code: Progressive Code Acceleration Through Bidirectional Tree Editing},
    author = {Longhui Zhang and Jiahao Wang and Meishan Zhang and GaoXiong Cao and Ensheng Shi and mayuchi and Jun Yu and Honghai LIU and Jing Li and  Min Zhang},
  booktitle = {The 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>





<confli > <strong>DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization</strong><br/>
      <i>Hexuan Deng, Wenxiang Jiao, Xuebo Liu, <strong>Jing Li</strong>, Min Zhang, Zhaopeng Tu<br/>
          <span class="abbpaper">ACL-25</span>- The 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2411.14055"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, a method that dynamically adjusts the data distribution during training to restore balanced performance across heterogeneous and multi-tasking data. Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. Further analysis demonstrates the robustness of DRPruning towards various domains and distribution shifts. Furthermore, DRPruning can determine optimal reference losses and data ratios automatically, suggesting potential for broader applications. 
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{hexuan_acl25,
  title={DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization},
    author = {Hexuan Deng and Wenxiang Jiao and Xuebo Liu and Jing Li and Min Zhang and Zhaopeng Tu},
  booktitle = {The 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>







<confli > <strong>Reflection on Knowledge Graph for Large Language Models Reasoning</strong><br/>
      <i>Yigeng Zhou, Wu Li, Yifan Lu, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Fangming Liu, Meishan Zhang, Yequan Wang, Daojing He, Honghai LIU, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- Findings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://aclanthology.org/2025.findings-acl.1221.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Recent research shows that supplementing Large Language Models (LLMs) with knowledge graphs can enhance their performance. However, existing methods often introduce noise in the retrieval and reasoning pipeline, hindering LLMs’ ability to effectively integrate external knowledge for complex multi-hop question answering. To address this, we propose RefKG, a novel framework designed to enhance the reasoning capabilities of LLMs through reflective engagement with knowledge graphs. RefKG autonomously conduct retrieval and reflection on knowledge graphs. It consists of three modules: Query Decoupling, LLM-Driven Knowledge Graph Exploration, and Inference with Knowledge Reconstruction. We also introduce a multi-task tuning strategy that not only integrates external knowledge into LLMs but also trains them to leverage this knowledge for answering questions. This significantly improves their performance on knowledge-intensive tasks. Experiments on fact verification and knowledge graph question answering demonstrate RefKG’s effectiveness.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{yigeng_acl25,
  title={Reflection on Knowledge Graph for Large Language Models Reasoning},
    author = {Yigeng Zhou and Wu Li and Yifan Lu and Jing Li and Fangming Liu and Meishan Zhang and Yequan Wang and Daojing He and Honghai LIU and Min Zhang},
  booktitle = {Findings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>



<confli > <strong>Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing</strong><br/>
      <i>Yifan Lu, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Yigeng Zhou, Yihui Zhang, Wenya Wang, Xiucheng Li, Meishan Zhang, Fangming Liu, Jun Yu, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- Findings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://aclanthology.org/2025.findings-acl.1013.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Large language models (LLMs) exhibit impressive language capabilities but remain vulnerable to malicious prompts and jailbreaking attacks. Existing knowledge editing methods for LLM detoxification face two major challenges. First, they often rely on entity-specific localization, making them ineffective against adversarial inputs without explicit entities. Second, these methods suffer from over-editing, where detoxified models reject legitimate queries, compromising overall performance. In this paper, we propose ToxEdit, a toxicity-aware knowledge editing approach that dynamically detects toxic activation patterns during forward propagation. It then routes computations through adaptive inter-layer pathways to mitigate toxicity effectively. This design ensures precise toxicity mitigation while preserving LLMs' general capabilities. To more accurately assess over-editing, we also enhance the SafeEdit benchmark by incorporating instruction-following evaluation tasks. Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms previous state-of-the-art methods in both detoxification performance and safeguarding general capabilities of LLMs.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{yifan_acl25,
  title={Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing},
    author = {Yifan Lu and Jing Li and Yigeng Zhou and Yihui Zhang and Wenya Wang and Xiucheng Li and Meishan Zhang and Fangming Liu and Jun Yu and Min Zhang},
  booktitle = {Findings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>



<confli > <strong>LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models</strong><br/>
      <i>Xinxin Li, Huiyao Chen, Chengjun Liu, <strong>Jing Li</strong>, Meishan Zhang, Jun Yu, Min Zhang<br/>
          <span class="abbpaper">ACL-25</span>- Findings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://arxiv.org/pdf/2506.05385"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Semantic role labeling (SRL) is a crucial task of natural language processing (NLP). Although generative decoder-based large language models (LLMs) have achieved remarkable success across various NLP tasks, they still lag behind state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a) retrieval-augmented generation and (b) self-correction. The first mechanism enables LLMs to leverage external linguistic knowledge such as predicate and argument structure descriptions, while the second allows LLMs to identify and correct inconsistent SRL outputs. We conduct extensive experiments on three widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results demonstrate that our method achieves state-of-the-art performance in both Chinese and English, marking the first successful application of LLMs to surpass encoder-decoder approaches in SRL.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{xinxin_acl25,
  title={LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models},
    author = {Xinxin Li and Huiyao Chen and Chengjun Liu and Jing Li and Meishan Zhang and Jun Yu and Min Zhang},
  booktitle = {Findings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>






<confli > <strong>ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation</strong><br/>
      <i>Kaiyuan Liu, Youcheng Pan, Yang Xiang, Daojing He, <strong>Jing Li</strong>, Yexing Du, Tianrun Gao<br/>
          <span class="abbpaper">ACL-25</span>- Findings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025 <br/>
          <a class="file_link" href="https://aclanthology.org/2025.findings-acl.1036.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href=""  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.
       </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@inproceedings{kaiyuan_acl25,
  title={ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation},
    author = {Kaiyuan Liu and Youcheng Pan and Yang Xiang and Daojing He and Jing Li and Yexing Du and Tianrun Gao},
  booktitle = {Findings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}
          </pre>
          </div>
    
    </i> </confli>








<confli > <strong>Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question Answering</strong><br/>
  <i>Yifan Lu, Yigeng Zhou, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Yequan Wang, Xuebo Liu, Daojing He, Fangming Liu, Min Zhang<br/>
      <span class="abbpaper">AAAI-25</span>- The Thirty-Ninth AAAI Conference on Artificial Intelligence, 2025. <br/>
      <a class="file_link" href="https://arxiv.org/abs/2412.13782"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

         <a class="file_link" href="https://github.com/ABi-dot/Kedkg"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_yifanaaai25').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_yifanaaai25').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_yifanaaai25" class="verbatim" style="display:none">
     Multi-hop question answering (MHQA) poses a significant challenge for large language models (LLMs) due the extensive knowledge demands involved. Knowledge editing, which aims to precisely modify the LLMs to incorporate specific knowledge without negatively impacting other unrelated knowledge, offers a potential solution for addressing MHQA challenges with LLMs. However, current solutions struggle to effectively resolve issues of knowledge conflicts. Most parameter-preserving editing methods are hindered by inaccurate retrieval and overlook secondary editing issues, which can introduce noise into the reasoning process of LLMs. In this paper, we introduce KEDKG, a novel knowledge editing method that leverages a dynamic knowledge graph for MHQA, designed to ensure the reliability of answers. KEDKG involves two primary steps: dynamic knowledge graph construction and knowledge graph augmented generation. Initially, KEDKG autonomously constructs a dynamic knowledge graph to store revised information while resolving potential knowledge conflicts. Subsequently, it employs a fine-grained retrieval strategy coupled with an entity and relation detector to enhance the accuracy of graph retrieval for LLM generation. Experimental results on benchmarks show that KEDKG surpasses previous state-of-the-art models, delivering more accurate and reliable answers in environments with dynamic information.
   </div>

      <div id="bibtex_yifanaaai25" style="display:none">
        <pre class="verbatim">
@inproceedings{kedkg_aaai_25,
  title={Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question Answering},
    author = {Yifan Lu and  Yigeng Zhou and Jing Li and Yequan Wang and  Xuebo Liu and Daojing He and Fangming Liu andMin Zhang},
  booktitle = {The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI)},
  year={2025}
}
      </pre>
      </div>

</i> </confli>







<confli > <strong>Impromptu Cybercrime Euphemism Detection</strong><br/>
  <i>Xiang Li, Yucheng Zhou, Laiping Zhao, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Fangming Liu <sup><i  style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup><br/>
      <span class="abbpaper">COLING-25</span>- The 31st International Conference on Computational Linguistics, 2025. <br/>
      <a class="file_link" href="https://arxiv.org/abs/2412.01413"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_coling25').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_coling25').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_coling25" class="verbatim" style="display:none">
      Detecting euphemisms is essential for content security on various social media platforms, but existing methods designed for detecting euphemisms are ineffective in impromptu euphemisms. In this work, we make a first attempt to an exploration of impromptu euphemism detection and introduce the Impromptu Cybercrime Euphemisms Detection (ICED) dataset. Moreover, we propose a detection framework tailored to this problem, which employs context augmentation modeling and multi-round iterative training. Our detection framework mainly consists of a coarse-grained and a fine-grained classification model. The coarse-grained classification model removes most of the harmless content in the corpus to be detected. The fine-grained model, impromptu euphemisms detector, integrates context augmentation and multi-round iterations training to better predicts the actual meaning of a masked token. In addition, we leverage ChatGPT to evaluate the mode's capability. Experimental results demonstrate that our approach achieves a remarkable 76-fold improvement compared to the previous state-of-the-art euphemism detector.
   </div>

      <div id="bibtex_coling25" style="display:none">
        <pre class="verbatim">
@inproceedings{xianglicoling25,
  title={Impromptu Cybercrime Euphemism Detection},
    author = {Xiang Li and Yucheng Zhou and Laiping Zhao and Jing Li and Fangming Liu},
  booktitle = {The 31st International Conference on Computational Linguistics (COLING)},
  year={2025}
}
      </pre>
      </div>

</i> </confli>





<li > <strong>MFG-SciSum: A multimodal faceted graph framework for scientific summarization</strong><br/>
        <i>Wenhui Yu, Zusheng Tan, Fan Yang, <strong>Jing Li</strong>, Shen Gao, Wai Lam, Sam Kwong, Billy Chiu<br/>
      <span class="abbpaper">INS-25</span>- Information Sciences, 2025. <br/>
          <a class="file_link" href="https://www.sciencedirect.com/science/article/pii/S0020025525009879"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/Kwanheiyu2001/MFG-SciSum"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Scientific papers are often organized into structured facets (e.g., Introduction, Methods), and modern research dissemination increasingly includes multimodal content such as presentation videos and audio. This shift creates a need for summarization systems that can effectively integrate both structured and multimodal information. In this paper, we introduce the Multimodal Faceted Graph Scientific Summarization model, a graph-based model for multimodal faceted summarization. At its core, is a Multimodal Faceted Graph that encodes fine-grained elements—text spans, visual segments, and audio snippets from both papers and presentations—as distinct node types. It constructs a cross-modal, multi-level graph through unsupervised alignment and latent content grouping, enabling coherent structural and semantic alignment. To enhance multimodal integration, we further incorporate a Heterogeneous Graph Refiner and a Heterogeneous Graph Condenser, which refine and distill salient information from the multimodal graph into summary-focused representations. The model is trained with a joint text-graph objective that promotes both summary quality and structural consistency. Experiments show that our model outperforms both uni- and multimodal baselines across automatic and human evaluations, demonstrating the effectiveness of graph-based cross-modal modeling for scientific summarization. The code for our model is available at: https://github.com/Kwanheiyu2001/MFG-SciSum  </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@article{mfgscisum_25,
  title={MFG-SciSum: A multimodal faceted graph framework for scientific summarization},
  author = {Wenhui Yu and Zusheng Tan and Fan Yang and Jing Li and Shen Gao and Wai Lam and Sam Kwong and Billy Chiu},
  booktitle = {Information Sciences},
  year={2025}
}
          </pre>
          </div>
    
    </i> </li>




<li > <strong>SMSMO: Learning to generate multimodal summary for scientific papers</strong><br/> 
      <i> Xinyi Zhong, Zusheng Tan, Shen Gao, <strong>Jing Li</strong>, Jiaxing Shen, Jingyu Ji, Jeff Tang, Billy Chiu<br/>
      <span class="abbpaper">ELSEVIER KBS-25</span>- Knowledge-Based Systems, Volume 310, 2025 <br/>

      <a class="file_link" href="https://kwnsfk27.r.eu-west-1.awstrack.me/L0/https:%2F%2Fauthors.elsevier.com%2Fc%2F1kNoI3OAb9C-r5/1/01020194318384ac-708ed87d-6d55-4ba1-9888-d80be98b223e-000000/eYOARt5_yNMgBI1x_3LYE6B-zHA=407"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_kbs25').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_kbs25').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>
      <br/>

      <div id="abstract_kbs25" class="verbatim" style="display:none">
Nowadays, publishers like Elsevier increasingly use graphical abstracts (i.e., a pictorial paper summary) along with textual abstracts to facilitate scientific paper readings. In such a case, automatically identifying a representative image and generating a suitable textual summary for individual papers can help editors and readers save time, facilitating them in reading and understanding papers. To tackle the case, we introduce the dataset for Scientific Multimodal Summarization with Multimodal Output (SMSMO). Unlike other multimodal tasks which performed on generic, medium-size contents (e.g., news), SMSMO needs to tackle longer multimodal contents in papers, with finer-grained multimodality interactions and semantic alignments between images and text. For this, we propose a cross-modality, multi-task learning summarizer (CMT-Sum). It captures the intra- and inter-modality interactions between images and text through a cross-fusion module; and models the finer-grained image–text semantic alignment by jointly generating the text summary, selecting the key image and matching the text and image. Extensive experiments conducted on two newly introduced datasets on the SMSMO task showcase our model’s effectiveness.
      </div>

      <div id="bibtex_kbs25" style="display:none">
      <pre class="verbatim">
@article{zhong2024smsmo,
  title={SMSMO: Learning to generate multimodal summary for scientific papers},
  author={Zhong, Xinyi and Tan, Zusheng and Gao, Shen and Li, Jing and Shen, Jiaxing and Ji, Jingyu and Tang, Jeff and Chiu, Billy},
  journal={Knowledge-Based Systems},
  pages={112908},
  year={2024},
  publisher={Elsevier}
}
      </pre>
      </div>

</i> </li>



<li > <strong>Scientific poster generation: A new dataset and approach</strong><br/>
        <i>Xinyi Zhong, Zusheng Tan, <strong>Jing Li</strong>, Shen Gao, Jing Ma, Shanshan Feng, Billy Chiu<br/>
      <span class="abbpaper">ELSEVIER PR</span>- Pattern Recognition, 2026. <br/>
          <a class="file_link" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320325001670"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 
    
             <a class="file_link" href="https://github.com/kitman0000/Sci-PosterLayout-Data"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 
    
          <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract').slideToggle('fast');return false;">
                                 <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 
    
          <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 
    
    
          <br/>
    
          <div id="abstract" class="verbatim" style="display:none">
Automating poster creation from research papers saves scientists time. However, training models for this task is challenging due to limited datasets. Moreover, existing methods are mostly rule/template-based, which lack the flexibility to adapt to different content and design requirements in scientific posters. Our contributions aim to address these issues. We introduce Sci-PosterLayout, a dataset comprising 1,226 scientific posters with greater variety in content, layout and domains. Using a template-free method with a seq2seq model and Design Pattern Schema (DPS), we learn various content and design patterns for poster layout generation. Evaluations against existing methods and datasets show our approach produces high-quality posters with diverse layouts. Our work seeks to advance research in scientific poster generation by building a new dataset and proposing template-free methods that require minimal human intervention. The Sci-PosterLayout dataset will be publicly available at https://github.com/kitman0000/Sci-PosterLayout-Data. </div>
    
          <div id="bibtex" style="display:none">
            <pre class="verbatim">
@article{dps_pr26,
  title={Scientific poster generation: A new dataset and approach},
  author = {Xinyi Zhong and Zusheng Tan and Jing Li and Shen Gao and Jing Ma and Shanshan Feng and Billy Chiu},
  booktitle = {Pattern Recognition},
  year={2026}
}
          </pre>
          </div>
    
    </i> </li>





<h3 class="year">2024</h3>

<confli > <strong>Parameter Competition Balancing for Model Merging</strong><br/>
  <i> Guodong Du, Junlin Lee, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang<br/>
      <span class="abbpaper">NeurIPS-24</span>- The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. <br/>
      <a class="file_link" href="https://arxiv.org/abs/2410.02396"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

        <a class="file_link" href="https://github.com/duguodong7/pcb-merging"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_nips24guodong').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_nips24guodong').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_nips24guodong" class="verbatim" style="display:none">
        While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. 
   </div>

      <div id="bibtex_nips24guodong" style="display:none">
        <pre class="verbatim">
@inproceedings{guodong24neurips,
  title={Parameter Competition Balancing for Model Merging},
    author = {Guodong Du and
    Junlin Lee and Jing Li  and Runhua Jiang and Yifei Guo and Shuyang Yu and Hanting Liu and Sim Kuan Goh and Ho-Kin Tang and Daojing He and Min Zhang},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}
      </pre>
      </div>

</i> </confli>






<confli > <strong>Multimodal Reasoning with Multimodal Knowledge Graph</strong><br/>
      <i> Junlin Lee, Yequan Wang, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> and Min Zhang<br/>
      <span class="abbpaper">ACL-24</span>- The 62nd Annual Meeting of the Association for Computational Linguistics, 2024. <br/>

      <a class="file_link" href="https://arxiv.org/abs/2406.02030"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_acl24junlin').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_cl24junlin').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_acl24junlin" class="verbatim" style="display:none">
        Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.
   </div>

      <div id="bibtex_cl24junlin" style="display:none">
        <pre class="verbatim">
@inproceedings{junlin24acl,
  title={Multimodal Reasoning with Multimodal Knowledge Graph},
    author       = {Junlin Lee and
                  Yequan Wang and
                  Jing Li and
                  Min Zhang},
  booktitle = {The 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024}
}
      </pre>
      </div>

</i> </confli>



<confli > <strong>Knowledge Fusion By Evolving Weights of Language Models</strong><br/>
      <i> Guodong Du, <strong>Jing Li</strong> <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup>, Hanting Liu, Runhua Jiang, Shuyang Yu, Yifei Guo, Sim Kuan Goh and Ho-Kin Tang<br/>
      <span class="abbpaper">ACL-24</span>- Findings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. <br/>

      <a class="file_link" href="https://arxiv.org/abs/2406.12208"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

      <a class="file_link" href="https://github.com/duguodong7/model-evolution"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_acl24du').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_cl24du').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_acl24du" class="verbatim" style="display:none">
        Fine-tuning pre-trained language models, particularly large language models, demands extensive computing resources and can result in varying performance outcomes across different domains and datasets. This paper examines the approach of integrating multiple models from diverse training scenarios into a unified model. This unified model excels across various data domains and exhibits the ability to generalize well on out-of-domain data. We propose a knowledge fusion method named Evolver, inspired by evolutionary algorithms, which does not need further training or additional training data. Specifically, our method involves aggregating the weights of different language models into a population and subsequently generating offspring models through mutation and crossover operations. These offspring models are then evaluated against their parents, allowing for the preservation of those models that show enhanced performance on development datasets. Importantly, our model evolving strategy can be seamlessly integrated with existing model merging frameworks, offering a versatile tool for model enhancement. Experimental results on mainstream language models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that Evolver outperforms previous state-of-the-art models by large margins. The code is publicly available at https://github.com/duguodong7/model-evolution.
   </div>

      <div id="bibtex_cl24du" style="display:none">
        <pre class="verbatim">
@inproceedings{guodong24acl,
  title={Knowledge Fusion By Evolving Weights of Language Models},
    author       = {Guodong DU and 
                     Jing Li and
                     Hanting Liu and
                     Runhua Jiang and 
                     Shuyang Yu and 
                     Yifei Guo and 
                     Sim Kuan Goh and 
                     Ho-Kin Tang},
  booktitle = {Findings of The 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024}
}
      </pre>
      </div>

</i> </confli>








<confli > <strong>Masked Structural Growth for 2x Faster Language Model Pre-training</strong><br/>
      <i> Yiqun Yao, Zheng Zhang, <strong>Jing Li</strong> and Yequan Wang <br/>
      <span class="abbpaper">ICLR-24</span>- The Twelfth International Conference on Learning Representations, 2024. <br/>

      <a class="file_link" href="https://openreview.net/pdf?id=rL7xsg1aRn"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 

       <a class="file_link" href="https://github.com/cofe-ai/MSG"  target="_blank" style="text-decoration:none;"> <i class="fa fa-code-fork"></i> Code</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_iclr24').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_iclr24').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_iclr24" class="verbatim" style="display:none">
Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve a speed-up of 80% for Bert-base and 120% for Bert-large pre-training. Moreover, MSG is able to improve fine-tuning performances at the same time.
   </div>

      <div id="bibtex_iclr24" style="display:none">
        <pre class="verbatim">
@inproceedings{yao24iclr,
  title={Masked Structural Growth for 2x Faster Language Model Pre-training},
    author       = {Yiqun Yao and
                  Zheng Zhang and
                  Jing Li and
                  Yequan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations (ICLR)},
  year={2024}
}
      </pre>
      </div>

</i> </confli>



<li > <strong>Few-Shot Relation Extraction With Dual Graph Neural Network Interaction</strong><br/>
      <i> <strong>Jing Li</strong>, Shanshan Feng and Billy Chiu<br/>
      <span class="abbpaper">IEEE TNNLS-24</span>-  IEEE Transactions on Neural Networks and Learning Systems, 35(10): 14396-14408, 2024.  <br/>

      <a class="file_link" href="papers/23tnnlsdualgraph.pdf" target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_tnnls23').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_tnnls23').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>
      <br/>

      <div id="abstract_tnnls23" class="verbatim" style="display:none">
 Recent advances in relation extraction with deep neural architectures have achieved excellent performance. However, current models still suffer from two main drawbacks: 1) they require enormous volumes of training data to avoid model overfitting and 2) there is a sharp decrease in performance when the data distribution during training and testing shift from one domain to the other. It is thus vital to reduce the data requirement in training and explicitly model the distribution difference when transferring knowledge from one domain to another. In this work, we concentrate on few-shot relation extraction under domain adaptation settings. Specifically, we propose, a novel graph neural network (GNN) based approach for few-shot relation extraction. leverages an edge-labeling dual graph (i.e. an instance graph and a distribution graph) to explicitly model the intraclass similarity and interclass dissimilarity in each individual graph, as well as the instance-level and distribution-level relations across graphs. A dual graph interaction mechanism is proposed to adequately fuse the information between the two graphs in a cyclic flow manner. We extensively evaluate on FewRel1.0 and FewRel2.0 benchmarks under four few-shot configurations. The experimental results demonstrate that can match or outperform previously published approaches. We also perform experiments to further investigate the parameter settings and architectural choices, and we offer a qualitative analysis.
      </div>

      <div id="bibtex_tnnls23" style="display:none">
      <pre class="verbatim">
@article{jing24dualgraph,
  title={Few-Shot Relation Extraction With Dual Graph Neural Network Interaction},
  author={Li, Jing and Feng, Shanshan and Chiu, Billy},
  journal={IEEE Transactions on Neural Networks and Learning Systems (TNNLS)},
  volume    = {35},
  number    = {10},
  pages     = {14396--14408},
  year      = {2024},
  publisher={IEEE}
}
      </pre>
      </div>

</i> </li>


<h3 class="year">Before 2023</h3>


<confli > <strong>Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction </strong><br/>
      <i>Xilai Ma, <strong>Jing Li</strong>  <sup><i style="color:#FF00FF" class="fa fa-envelope-o fa-1x"/></i></sup> and Min Zhang<br/>
      <span class="abbpaper">EMNLP-23</span>- Findings of The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. <br/>

      <a class="file_link" href="https://aclanthology.org/2023.findings-emnlp.153.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_emnlp23').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_emnlp23').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_emnlp23" class="verbatim" style="display:none">
Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-specific and concept-level knowledge. Then these evidences are explicitly incorporated into chain-of-thought prompting for relation extraction. Experimental results demonstrate that our CoT-ER approach (with 0% training data) achieves competitive performance compared to the fully-supervised (with 100% training data) state-of-the-art approach on the FewRel1.0 and FewRel2.0 datasets.
   </div>

      <div id="bibtex_emnlp23" style="display:none">
        <pre class="verbatim">
@inproceedings{DBLP:conf/emnlp/MaLZ23a,
  author       = {Xilai Ma and
                  Jing Li and
                  Min Zhang},
  title        = {Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation
                  Extraction},
  booktitle    = {Findings of the Association for Computational Linguistics (EMNLP),
  pages        = {2334--2352},
  year         = {2023},
  url          = {https://aclanthology.org/2023.findings-emnlp.153},
}
      </pre>
      </div>

</i> </confli>





<confli > <strong>Rethinking Document-Level Relation Extraction: A Reality Check </strong><br/>
      <i> <strong>Jing Li</strong>, Yequan Wang, Shuai Zhang and Min Zhang<br/>
      <span class="abbpaper">ACL-23</span>- Findings of The 61st Annual Meeting of the Association for Computational Linguistics, 2023. <br/>

      <a class="file_link" href="https://arxiv.org/pdf/2306.08953.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_acl23').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_acl23').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_acl23" class="verbatim" style="display:none">
Recently, numerous efforts have continued to push up performance boundaries of document-level relation extraction (DocRE) and have claimed significant progress in DocRE. In this paper, we do not aim at proposing a novel model for DocRE. Instead, we take a closer look at the field to see if these performance gains are actually true. By taking a comprehensive literature review and a thorough examination of popular DocRE datasets, we find that these performance gains are achieved upon a strong or even untenable assumption in common: all named entities are perfectly localized, normalized, and typed in advance. Next, we construct four types of entity mention attacks to examine the robustness of typical DocRE models by behavioral probing. We also have a close check on model usability in a more realistic setting. Our findings reveal that most of current DocRE models are vulnerable to entity mention attacks and difficult to be deployed in real-world end-user NLP applications. Our study calls more attentions for future research to stop simplifying problem setups, and to model DocRE in the wild rather than in an unrealistic Utopian world.
   </div>

      <div id="bibtex_acl23" style="display:none">
        <pre class="verbatim">
@inproceedings{li2023rethinking,
  title={Rethinking Document-Level Relation Extraction: A Reality Check},
  author={Li, Jing and Wang, Yequan and Zhang, Shuai and Zhang, Min},
  pages= {5715--5730},
  booktitle = {Findings of The 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2023}
}
      </pre>
      </div>

</i> </confli>










<confli > <strong>Few-Shot Named Entity Recognition via Meta-Learning (Extended Abstract)</strong><br/>
      <i> <strong>Jing Li</strong>, Billy Chiu, Shanshan Feng and Hao Wang<br/>
      <span class="abbpaper">ICDE-23</span>- The 39th IEEE International Conference on Data Engineering, 2023. <br/>

      <a class="file_link" href="https://ieeexplore.ieee.org/document/10184641/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_icde23_few').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_icde23_few').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_icde23_few" class="verbatim" style="display:none">
toupdate
   </div>

      <div id="bibtex_icde23_few" style="display:none">
   toupdate
      </div>

</i> </confli>





<confli > <strong>A Survey on Deep Learning for Named Entity Recognition (Extended Abstract)</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun, Jianglei Han and Chenliang Li<br/>
      <span class="abbpaper">ICDE-23</span>- The 39th IEEE International Conference on Data Engineering, 2023. <br/>

      <a class="file_link" href="https://ieeexplore.ieee.org/document/10184827"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_icde23_ner').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_icde23_ner').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_icde23_ner" class="verbatim" style="display:none">
toupdate
   </div>

      <div id="bibtex_icde23_ner" style="display:none">
   toupdate
      </div>

</i> </confli>








<confli > <strong>GRLSTM: Trajectory Similarity Computation with Graph-based Residual LSTM</strong><br/>
      <i>Silin Zhou, <strong>Jing Li</strong>, Hao Wang, Shuo Shang, Peng Han<br/>
      <span class="abbpaper">AAAI-23</span>- The Thirty-Seventh AAAI Conference on Artificial Intelligence. <br/>

      <a class="file_link" href="https://ojs.aaai.org/index.php/AAAI/article/view/25624/25396"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_23GRLSTM').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_23GRLSTM').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_23GRLSTM" class="verbatim" style="display:none">
The computation of trajectory similarity is a crucial task in many spatial data analysis applications. However, existing methods have been designed primarily for trajectories in Euclidean space, which overlooks the fact that real-world trajectories are often generated on road networks. This paper addresses this gap by proposing a novel framework, called GRLSTM (Graph-based Residual LSTM). To jointly capture the properties of trajectories and road networks, the proposed framework incorporates knowledge graph embedding (KGE), graph neural network (GNN), and the residual network into the multi-layer LSTM (Residual-LSTM). Specifically, the framework constructs a point knowledge graph to study the multi-relation of points, as points may belong to both the trajectory and the road network. KGE is introduced to learn point embeddings and relation embeddings to build the point fusion graph, while GNN is used to capture the topology structure information of the point fusion graph. Finally, Residual-LSTM is used to learn the trajectory embeddings.To further enhance the accuracy and robustness of the final trajectory embeddings, we introduce two new neighbor-based point loss functions, namely, graph-based point loss function and trajectory-based point loss function. The GRLSTM is evaluated using two real-world trajectory datasets, and the experimental results demonstrate that GRLSTM outperforms all the state-of-the-art methods significantly.
   </div>

      <div id="bibtex_23GRLSTM" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/aaai/Zhou0WS023,
  author       = {Silin Zhou and
                  Jing Li and
                  Hao Wang and
                  Shuo Shang and
                  Peng Han},
  editor       = {Brian Williams and
                  Yiling Chen and
                  Jennifer Neville},
  title        = {{GRLSTM:} Trajectory Similarity Computation with Graph-Based Residual
                  {LSTM}},
  booktitle    = {Thirty-Seventh {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2023, Thirty-Fifth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2023, Thirteenth Symposium on Educational Advances
                  in Artificial Intelligence},
  pages        = {4972--4980},
  year         = {2023},
}
      </pre>
      </div>

</i> </confli>





<li > <strong>Sequence Labeling with Meta-Learning</strong><br/> 
      <i> <strong>Jing Li</strong>, Peng Han, Xiangnan Ren, Jilin Hu, Lisi Chen and Shuo Shang<br/>
      <span class="abbpaper">IEEE TKDE-23</span>- IEEE Transactions on Knowledge and Data Engineering, 35(3): 3072-3086, 2023. <br/>

      <a class="file_link" href="papers/21metaseq.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_seq21').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_seq21').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>
      <br/>

      <div id="abstract_seq21" class="verbatim" style="display:none">
Recent neural architectures in sequence labeling have yielded state-of-the-art performance on single domain data such as newswires. However, they still suffer from (i) requiring massive amounts of training data to avoid overfitting; (ii) huge performance degradation when there is a domain shift in the data distribution between training and testing. In this paper, we investigate the problem of domain adaptation for sequence labeling under homogeneous and heterogeneous settings. We propose MetaSeq, a novel meta-learning approach for domain adaptation in sequence labeling. Specifically, MetaSeq incorporates meta-learning and adversarial training strategies to encourage robust, general and transferable representations for sequence labeling. The key advantage of MetaSeq is that it is capable of adapting to new unseen domains with a small amount of annotated data from those domains. We extensively evaluate MetaSeq on named entity recognition, part-of-speech tagging and slot filling tasks under homogeneous and heterogeneous settings. The experimental results show that MetaSeq achieves state-of-the-art performance against eight baselines. Impressively, MetaSeq surpasses the in-domain performance using only 16.17% and 7% of target domain data on average for homogeneous settings, and 34.76%, 24%, 22.5% of target domain data on average for heterogeneous settings.
      </div>

      <div id="bibtex_seq21" style="display:none">
      <pre class="verbatim">
@article{jing23seq,
author    = {Jing Li and Peng Han and Xiangnan Ren and Jilin Hu and Lisi Chen and Shuo Shang},
title     = {Sequence Labeling with Meta-Learning},
journal   = {IEEE Transactions on Knowledge and Data Engineering (TKDE)},
volume       = {35},
number       = {3},
pages        = {3072--3086},
year         = {2023},
url       = {https://doi.org/10.1109/TKDE.2021.3118469},
doi       = {10.1109/TKDE.2021.3118469},
}
      </pre>
      </div>

</i> </li>




<confli > <strong>A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict</strong><br/>
      <i>Yiyi Liu, Yequan Wang, Aixin Sun, Xuying Meng, <strong>Jing Li</strong>, Jiafeng Guo<br/>
      <span class="abbpaper">NAACL-22</span>- Findings of 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics. <br/>

      <a class="file_link" href="https://arxiv.org/pdf/2109.03587.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_22dcnet').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_22dcnet').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_22dcnet" class="verbatim" style="display:none">
Sarcasm employs ambivalence, where one says something positive but actually means negative, and vice versa. The essence of sarcasm, which is also a sufficient and necessary condition, is the conflict between literal and implied sentiments expressed in one sentence. However, it is difficult to recognize such sentiment conflict because the sentiments are mixed or even implicit. As a result, the
recognition of sophisticated and obscure sentiment brings in a great challenge to sarcasm detection. In this paper, we propose a DualChannel Framework by modeling both literal and implied sentiments separately. Based on this dual-channel framework, we design the Dual-Channel Network (DC-Net) to recognize sentiment conflict. Experiments on political debates (i.e., IAC-V1 and IAC-V2) and Twitter datasets show that our proposed DC-Net achieves state-of-the-art performance on sarcasm recognition. Our code is released to support research https://github.com/yiyi-ict/dual-channel-for-sarcasm.
   </div>

      <div id="bibtex_22dcnet" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/naacl/LiuWSMLG22,
  author    = {Yiyi Liu and
               Yequan Wang and
               Aixin Sun and
               Xuying Meng and
               Jing Li and
               Jiafeng Guo},
  editor    = {Marine Carpuat and
               Marie{-}Catherine de Marneffe and
               Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
  title     = {A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment
               Conflict},
  booktitle = {Findings of the Association for Computational Linguistics: {NAACL}
               2022, Seattle, WA, United States, July 10-15, 2022},
  pages     = {1670--1680},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.findings-naacl.126},
  doi       = {10.18653/v1/2022.findings-naacl.126},
  timestamp = {Tue, 31 Jan 2023 17:06:57 +0100},
  biburl    = {https://dblp.org/rec/conf/naacl/LiuWSMLG22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
      </pre>
      </div>

</i> </confli>





<confli > <strong>Interactive Information Extraction by Semantic Information Graph </strong><br/>
      <i>Siqi Fan, Yequan Wang, <strong>Jing Li</strong>, Zheng Zhang, Shuo Shang, Peng Han<br/>
      <span class="abbpaper">IJCAI-ECAI-22</span>- The 31st International Joint Conference on Artificial Intelligence and the 25th European Conference on Artificial Intelligence, 2022. Acceptance rate: 15%. <br/>

      <a class="file_link" href="https://www.ijcai.org/proceedings/2022/0569.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_ijcai22ie').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_ijcai22ie').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_ijcai22ie" class="verbatim" style="display:none">
Information extraction (IE) mainly focuses on three highly correlated subtasks, i.e., entity extraction, relation extraction and event extraction. Recently, there are studies using Abstract Meaning Representation (AMR) to utilize the intrinsic correlations among these three subtasks. AMR based models are capable of building the relationship of arguments. However, they are hard to deal with relations. In addition, the noises of AMR (i.e., tags unrelated to IE tasks, nodes with unconcerned conception, and edge types with complicated hierarchical structures) disturb the decoding processing of IE. As a result, the decoding processing limited by the AMR cannot be worked effectively. To overcome the shortages, we propose an Interactive Information Extraction (InterIE) model based on a novel Semantic Information Graph (SIG). SIG can guide our InterIE model to tackle the three subtasks jointly. Furthermore, the well-designed SIG without noise is capable of enriching entity and event trigger representation, and capturing the edge connection between the information types. Experimental results show that our InterIE achieves state-of-the-art performance on all IE subtasks on the benchmark dataset (i.e., ACE05-E+ and ACE05-E). More importantly, the proposed model is not sensitive to the decoding order, which goes beyond the limitations of AMR based methods.
   </div>

      <div id="bibtex_ijcai22ie" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/ijcai/FanWLZSH22,
  author    = {Siqi Fan and
               Yequan Wang and
               Jing Li and
               Zheng Zhang and
               Shuo Shang and
               Peng Han},
  editor    = {Luc De Raedt},
  title     = {Interactive Information Extraction by Semantic Information Graph},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI} 2022, Vienna, Austria, 23-29 July
               2022},
  pages     = {4100--4106},
  publisher = {ijcai.org},
  year      = {2022},
  url       = {https://doi.org/10.24963/ijcai.2022/569},
  doi       = {10.24963/ijcai.2022/569},
  timestamp = {Wed, 27 Jul 2022 16:43:00 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/FanWLZSH22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
      </pre>
      </div>

</i> </confli>





<confli > <strong>FOGS: First-Order Gradient Supervision with Learning-based Graph for Traffic Flow Forecasting</strong><br/>
      <i>Xuan Rao, Hao Wang, Shuo Shang, Liang Zhang, <strong>Jing Li</strong>, Peng Han<br/>
      <span class="abbpaper">IJCAI-ECAI-22</span>- The 31st International Joint Conference on Artificial Intelligence and the 25th European Conference on Artificial Intelligence, 2022. Acceptance rate: 15%. <br/>

      <a class="file_link" href="https://www.ijcai.org/proceedings/2022/0545.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_ijcai22fogs').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_ijcai22fogs').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_ijcai22fogs" class="verbatim" style="display:none">
Traffic flow forecasting plays a vital role in the transportation domain. Existing studies usually manually construct correlation graphs and design sophisticated models for learning spatial and temporal features to predict future traffic states. However, manually constructed correlation graphs cannot accurately extract the complex patterns hidden in the traffic data. In addition, it is challenging for the prediction model to fit traffic data due to its irregularly-shaped distribution. To solve the above-mentioned problems, in this paper, we propose a novel learning-based method to learn a spatial-temporal correlation graph, which could make good use of the traffic flow data. Moreover, we propose First-Order Gradient Supervision (FOGS), a novel method for traffic flow forecasting. FOGS utilizes first-order gradients, rather than specific flows, to train prediction model, which effectively avoids the problem of fitting irregularly-shaped distributions. Comprehensive numerical evaluations on four real-world datasets reveal that the proposed methods achieve state-of-the-art performance and significantly outperform the benchmarks.
   </div>

      <div id="bibtex_ijcai22fogs" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/ijcai/RaoWZLS022,
  author    = {Xuan Rao and
               Hao Wang and
               Liang Zhang and
               Jing Li and
               Shuo Shang and
               Peng Han},
  editor    = {Luc De Raedt},
  title     = {{FOGS:} First-Order Gradient Supervision with Learning-based Graph
               for Traffic Flow Forecasting},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI} 2022, Vienna, Austria, 23-29 July
               2022},
  pages     = {3926--3932},
  publisher = {ijcai.org},
  year      = {2022},
  url       = {https://doi.org/10.24963/ijcai.2022/545},
  doi       = {10.24963/ijcai.2022/545},
  timestamp = {Sun, 02 Oct 2022 16:08:04 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/RaoWZLS022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
      </pre>
      </div>

</i> </confli>













<li > <strong>A Survey on Deep Learning for Named Entity Recognition</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun, Jianglei Han and Chenliang Li<br/>
      <span class="abbpaper">IEEE TKDE-22</span>- IEEE Transactions on Knowledge and Data Engineering, 34(1): 50-70, 2022. <br/>

       <a class="file_link" href="papers/22nersurvey.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_survey20').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_survey20').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>
      <br/>

      <div id="abstract_survey20" class="verbatim" style="display:none">
    Named entity recognition (NER) is the task to identify text spans that mention named entities, and to classify them into predefined categories such as person, location, organization etc. NER serves as the basis for a variety of natural language applications such as question answering, text summarization, and machine translation. Although early NER systems are successful in producing decent recognition accuracy, they often require much human effort in carefully designing rules or features. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding state-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.
      </div>

      <div id="bibtex_survey20" style="display:none">
      <pre class="verbatim">
@article{jing22nersurvey,
author    = {Jing Li and Aixin Sun and Jianglei Han and Chenliang Li},
title     = {A Survey on Deep Learning for Named Entity Recognition},
journal   = {IEEE Transactions on Knowledge and Data Engineering (TKDE)},
volume    = {34},
number    = {1},
pages     = {50--70},
year      = {2022},
url       = {https://doi.org/10.1109/TKDE.2020.2981314},
doi       = {10.1109/TKDE.2020.2981314},
}
      </pre>
      </div>

</i> </li>




<li > <strong>Neural Text Segmentation and Its Application to Sentiment Analysis</strong><br/>
      <i> <strong>Jing Li</strong>, Billy Chiu, Shuo Shang and Ling Shao<br/>
      <span class="abbpaper">IEEE TKDE-22</span>- IEEE Transactions on Knowledge and Data Engineering, 34(2): 828-842, 2022. <br/>

      <a class="file_link" href="papers/21Segsentiment.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_sentiment20').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_sentiment20').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> &nbsp 

      <a class="file_link" href="http://138.197.118.157:8000/segbot/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> Demo</a>  


      <br/>

      <div id="abstract_sentiment20" class="verbatim" style="display:none">
Text segmentation is a fundamental task in natural language processing. Depending on the levels of granularity, the task can be defined as segmenting a document into topical segments, or segmenting a sentence into elementary discourse units (EDUs). Traditional solutions to the two tasks heavily rely on carefully designed features. The recently proposed neural models do not need manual feature engineering, but they either suffer from sparse boundary tags or cannot efficiently handle the issue of variable size output vocabulary. In light of such limitations, we propose a generic end-to-end segmentation model, namely SEGBOT, which first uses a bidirectional recurrent neural network to encode an input text sequence. SEGBOT then uses another recurrent neural networks, together with a pointer network, to select text boundaries in the input sequence. In this way, SEGBOT does not require any hand-crafted features. More importantly, SEGBOT inherently handles the issue of variable size output vocabulary and the issue of sparse boundary tags. In our experiments, SEGBOT outperforms state-of-the-art models on two tasks: document-level topic segmentation and sentence-level EDU segmentation. As a downstream application, we further propose a hierarchical attention model for sentence-level sentiment analysis based on the outcomes of SEGBOT. The hierarchical model can make full use of both word-level and EDU-level information simultaneously for sentence-level sentiment analysis. In particular, it can effectively exploit EDU-level information, such as the inner properties of EDUs, which cannot be fully encoded in word-level features. Experimental results show that our hierarchical model achieves new state-of-the-art results on the Movie Review and Stanford Sentiment Treebank benchmarks.
   </div>

      <div id="bibtex_sentiment20" style="display:none">
      <pre class="verbatim">
@article{li22segsenti,
author    = {Jing Li and Billy Chiu and Shuo Shang and Ling Shao},
title     = {Neural Text Segmentation and Its Application to Sentiment Analysis},
journal   = {IEEE Transactions on Knowledge and Data Engineering (TKDE)},
volume    = {34},
number    = {2},
pages     = {828--842},
year      = {2022},
url       = {https://doi.org/10.1109/TKDE.2020.2983360},
doi       = {10.1109/TKDE.2020.2983360},
}
      </pre>
      </div>

</i> </li>







<li > <strong>Few-Shot Named Entity Recognition via Meta-Learning</strong><br/>
      <i> <strong>Jing Li</strong>, Billy Chiu, Shanshan Feng and Hao Wang<br/>
      <span class="abbpaper">IEEE TKDE-22</span>- IEEE Transactions on Knowledge and Data Engineering, 34(9): 4245-4256, 2022. <br/>

      <a class="file_link" href="papers/20TKDEfewshot.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_fewshot20').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_fewshot20').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>
      <br/>

      <div id="abstract_fewshot20" class="verbatim" style="display:none">
Few-shot learning under the N-way K-shot setting (i.e., K annotated samples for each of N classes) has been widely studied in relation extraction (e.g., FewRel) and image classification (e.g., Mini-ImageNet). Named entity recognition (NER) is typically framed as a sequence labeling problem where the entity classes are inherently entangled together because the entity number and classes in a sentence are not known in advance, leaving the N-way K-shot NER problem so far unexplored. In this paper, we first formally define a more suitable N-way K-shot setting for NER. Then we propose FewNER, a novel meta-learning approach for few-shot NER. FewNER separates the entire network into a task-independent part and a task-specific part. During training in FewNER, the task-independent part is meta-learned across multiple tasks and a task-specific part is learned for each single task in a low-dimensional space. At test time, FewNER keeps the task-independent part fixed and adapts to a new task via gradient descent by updating only the task-specific part, resulting in it being less prone to overfitting and more computationally efficient. The results demonstrate that FewNER achieves state-of-the-art performance against nine baseline methods by significant margins on three adaptation experiments.
   </div>

      <div id="bibtex_fewshot20" style="display:none">
      <pre class="verbatim">
@article{li20fewshot,
author    = {Jing Li and Billy Chiu and Shanshan Feng and Hao Wang},
title     = {Few-Shot Named Entity Recognition via Meta-Learning},
journal   = {IEEE Transactions on Knowledge and Data Engineering (TKDE)},
volume       = {34},
number       = {9},
pages        = {4245--4256},
year         = {2022},
url       = {https://doi.org/10.1109/TKDE.2020.3038670},
doi       = {10.1109/TKDE.2020.3038670},
}
      </pre>
      </div>

</i> </li>







<li > <strong>Neural Named Entity Boundary Detection</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun and Yukun Ma<br/>
      <span class="abbpaper">IEEE TKDE-21</span>- IEEE Transactions on Knowledge and Data Engineering, 33(4): 1790-1795, 2021. <br/>

      <a class="file_link" href="papers/21boundary.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_bdrybot20').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_bdrybot20').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> &nbsp 

      <a class="file_link" href="http://138.197.118.157:8000/bdrybot/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> Demo</a>  


      <br/>

      <div id="abstract_bdrybot20" class="verbatim" style="display:none">
In this paper, we focus on named entity boundary detection , which is to detect the start and end boundaries of an entity mention in text, without predicting its type. The detected entities are input to entity linking or fine-grained typing systems for semantic enrichment. We propose BdryBot , a recurrent neural network encoder-decoder framework with a pointer network to detect entity boundaries from a given sentence. The encoder considers both character-level representations and word-level embeddings to represent the input words. In this way, BdryBot does not require any hand-crafted features. Because of the pointer network, BdryBot overcomes the problem of variable size output vocabulary and the issue of sparse boundary tags. We conduct two sets of experiments, in-domain detection and cross-domain detection, on six datasets. Our results show that BdryBot achieves state-of-the-art performance against five baselines. In addition, our proposed approach can be further enhanced when incorporating contextualized language embeddings into token representations.
   </div>

      <div id="bibtex_bdrybot20" style="display:none">
      <pre class="verbatim">
@article{li21bdrybot,
author    = {Jing Li and Aixin Sun and Yukun Ma},
title     = {Neural Named Entity Boundary Detection},
journal   = {IEEE Transactions on Knowledge and Data Engineering (TKDE)},
volume    = {33},
number    = {4},
pages     = {1790--1795},
year      = {2021},
url       = {https://doi.org/10.1109/TKDE.2020.2981329},
doi       = {10.1109/TKDE.2020.2981329},
}
      </pre>
      </div>

</i> </li>









<li > <strong>Domain Generalization for Named Entity Boundary Detection via Meta-Learning</strong><br/>
      <i> <strong>Jing Li</strong>, Shuo Shang and Lisi Chen<br/>
      <span class="abbpaper">IEEE TNNLS-21</span>- IEEE Transactions on Neural Networks and Learning Systems, 32(9): 3819-3830, 2021. <br/>

      <a class="file_link" href="papers/21tnnls.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_domainG20').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_domainG20').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_domainG20" class="verbatim" style="display:none">
Named entity recognition (NER) aims to recognize mentions of rigid designators from text belonging to predefined semantic types, such as person, location, and organization. In this article, we focus on a fundamental subtask of NER, named entity boundary detection, which aims at detecting the start and end boundaries of an entity mention in the text, without predicting its semantic type. The entity boundary detection is essentially a sequence labeling problem. Existing sequence labeling methods either suffer from sparse boundary tags (i.e., entities are rare and nonentities are common) or they cannot well handle the issue of variable size output vocabulary (i.e., need to retrain models with respect to different vocabularies). To address these two issues, we propose a novel entity boundary labeling model that leverages pointer networks to effectively infer boundaries depending on the input sequence. On the other hand, training models on source domains that generalize to new target domains at the test time are a challenging problem because of the performance degradation. To alleviate this issue, we propose METABDRY, a novel domain generalization approach for entity boundary detection without requiring any access to target domain information. Especially, adversarial learning is adopted to encourage domain-invariant representations. Meanwhile, metalearning is used to explicitly simulate a domain shift during training so that metaknowledge from multiple resource domains can be effectively aggregated. As such, METABDRY explicitly optimizes the capability of ``learning to generalize,'' resulting in a more general and robust model to reduce the domain discrepancy. We first conduct experiments to demonstrate the effectiveness of our novel boundary labeling model. We then extensively evaluate METABDRY on eight data sets under domain generalization settings. The experimental results show that METABDRY achieves state-of-the-art results against the recent seven baselines.
   </div>

      <div id="bibtex_domainG20" style="display:none">
      <pre class="verbatim">
@article{li21domaingen,
author    = {Jing Li and Shuo Shang and Lisi Chen},
title     = {Domain Generalization for Named Entity Boundary Detection via Metalearning},
journal   = {IEEE Transactions on Neural Networks and Learning Systems (TNNLS)},
volume    = {32},
number    = {9},
pages     = {3819--3830},
year      = {2021},
url       = {https://doi.org/10.1109/TNNLS.2020.3015912},
doi       = {10.1109/TNNLS.2020.3015912},
}
      </pre>
      </div>

</i> </li>





<li  > <strong>Leveraging Official Content and Social Context to Recommend Software Documentation</strong><br/>
      <i> <strong>Jing Li</strong>, Zhenchang Xing  and Muhammad Ashad Kabir<br/>
      <span class="abbpaper">IEEE TSC-21</span>- IEEE Transactions on Services Computing, 14(2), 472-486, 2021. <br/>

      <a class="file_link" href="papers/21TSC.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_tsc18').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_tsc18').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_tsc18" class="verbatim" style="display:none">
For an unfamiliar Application Programming Interface (API), software developers often access the official documentation to learn its usage, and post questions related to this API on social question and answering (Q&A) sites to seek solutions. The official software documentation often captures the information about functionality and parameters, but lacks detailed descriptions in different usage scenarios. On the contrary, the discussions about APIs on social Q&A sites provide enriching usages. Moreover, existing code search engines and information retrieval systems cannot effectively return relevant software documentation when the issued query does not contain code snippets or API-like terms. In this paper, we present CnCxL2R , a software documentation recommendation strategy incorporating the content of official documentation and the social context on Q&A into a learning-to-rank schema. In the proposed strategy, the content, local context and global context of documentation are considered to select candidate documents. Then four types of features are extracted to learn a ranking model. We conduct a large-scale automatic evaluation on Java documentation recommendation. The results show that CnCxL2R achieves state-of-the-art performance over the eight baseline models. We also compare the CnCxL2R with Google search. The results show that CnCxL2R can recommend more relevant software documentation, and can effectively capture the semantic between the high-level intent in developers’ queries and the low-level implementation in software documentation.
     </div>

      <div id="bibtex_tsc18" style="display:none">
      <pre class="verbatim">
@article{TSCLiXK21,
  author    = {Jing Li and
               Zhenchang Xing and
               Muhammad Ashad Kabir},
  title     = {Leveraging Official Content and Social Context to Recommend Software
               Documentation},
  journal   = {{IEEE} Trans. Serv. Comput.},
  volume    = {14},
  number    = {2},
  pages     = {472--486},
  year      = {2021},
  url       = {https://doi.org/10.1109/TSC.2018.2812729},
  doi       = {10.1109/TSC.2018.2812729},
}
      </pre>
      </div>

</i> </li>
















<confli > <strong>HME: A Hyperbolic Metric Embedding Approach for Next-POI Recommendation </strong><br/>
      <i>Shanshan Feng, Lucas Vinh Tran, Gao Cong, Lisi Chen, <strong>Jing Li</strong> and Fan Li<br/>
      <span class="abbpaper">SIGIR-20</span>- The 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020. Acceptance rate: 147/555 (26%). <br/>

      <a class="file_link" href="papers/20sigirHME.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_hme').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_hme').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_hme" class="verbatim" style="display:none">
With the increasing popularity of location-aware social media services, next-Point-of-Interest (POI) recommendation has gained significant research interest. The key challenge of next-POI recommendation is to precisely learn users' sequential movements from sparse check-in data. To this end, various embedding methods have been proposed to learn the representations of check-in data in the Euclidean space. However, their ability to learn complex patterns, especially hierarchical structures, is limited by the dimensionality of the Euclidean space. To this end, we propose a new research direction that aims to learn the representations of check-in activities in a hyperbolic space, which yields two advantages. First, it can effectively capture the underlying hierarchical structures, which are implied by the power-law distributions of user movements. Second, it provides high representative strength and enables the check-in data to be effectively represented in a low-dimensional space. Specifically, to solve the next-POI recommendation task, we propose a novel hyperbolic metric embedding (HME) model, which projects the check-in data into a hyperbolic space. The HME jointly captures sequential transition, user preference, category and region information in a unified approach by learning embeddings in a shared hyperbolic space. To the best of our knowledge, this is the first study to explore a non-Euclidean embedding model for next-POI recommendation. We conduct extensive experiments on three check-in datasets to demonstrate the superiority of our hyperbolic embedding approach over the state-of-the-art next-POI recommendation algorithms. Moreover, we conduct experiments on another four online transaction datasets for next-item recommendation to further demonstrate the generality of our proposed model.
   </div>

      <div id="bibtex_hme" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/sigir/FengTCCLL20,
  author    = {Shanshan Feng and
               Lucas Vinh Tran and
               Gao Cong and
               Lisi Chen and
               Jing Li and
               Fan Li},
  title     = {{HME:} {A} Hyperbolic Metric Embedding Approach for Next-POI Recommendation},
  booktitle = {Proceedings of the 43rd International {ACM} {SIGIR} conference on
               research and development in Information Retrieval (SIGIR)},
  pages     = {1429--1438},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3397271.3401049},
  doi       = {10.1145/3397271.3401049},
}
      </pre>
      </div>

</i> </confli>







<confli > <strong>Contextualized Point-of-Interest Recommendation  </strong><br/>
      <i> Peng Han, Zhongxiao Li, Yong Liu, Peilin Zhao, <strong>Jing Li</strong>, Hao Wang and Shuo Shang<br/>
      <span class="abbpaper">IJCAI-PRICAI-20</span>- The 29th International Joint Conference on Artificial Intelligence and the 17th Pacific Rim International Conference on Artificial Intelligence, 2020. 
Acceptance rate: 592/4717 (12.6%). <br/>

      <a class="file_link" href="https://www.ijcai.org/Proceedings/2020/0344.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_cpoi').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_cpoi').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_cpoi" class="verbatim" style="display:none">
Point-of-interest (POI) recommendation has become an increasingly important sub-field of recommendation system research. Previous methods employ various assumptions to exploit the contextual information for improving the recommendation accuracy. The common property among them is that similar users are more likely to visit similar POIs and similar POIs would like to be visited by the same user. However, none of existing methods utilize similarity explicitly to make recommendations. In this paper, we propose a new framework for POI recommendation, which explicitly utilizes similarity with contextual information. Specifically, we categorize the context information into two groups, i.e., global and local context, and develop different regularization terms to incorporate them for recommendation. A graph Laplacian regularization term is utilized to exploit the global context information. Moreover, we cluster users into different groups, and let the objective function constrain the users in the same group to have similar predicted POI ratings. An alternating optimization method is developed to optimize our model and get the final rating matrix. The results in our experiments show that our algorithm outperforms all the state-of-the-art methods.
   </div>

      <div id="bibtex_cpoi" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/ijcai/HanLLZLWS20,
  author    = {Peng Han and
               Zhongxiao Li and
               Yong Liu and
               Peilin Zhao and
               Jing Li and
               Hao Wang and
               Shuo Shang},
  title     = {Contextualized Point-of-Interest Recommendation},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence (IJCAI)},
  pages     = {2484--2490},
  year      = {2020},
  url       = {https://doi.org/10.24963/ijcai.2020/344},
  doi       = {10.24963/ijcai.2020/344},
}
      </pre>
      </div>

</i> </confli>


























<confli > <strong>MetaNER: Named Entity Recognition with Meta-Learning </strong><br/>
      <i> <strong>Jing Li</strong>, Shuo Shang and Ling Shao<br/>
      <span class="abbpaper">WWW-20</span>- The Web Conference, 2020. Acceptance rate: 217/1129 (19.2%). <br/>

      <a class="file_link" href="papers/20MetaNER.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_metaner').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_metaner').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_metaner" class="verbatim" style="display:none">
Recent neural architectures in named entity recognition (NER) have yielded state-of-the-art performance on single domain data such as newswires. However, they still suffer from (i) requiring massive amounts of training data to avoid overfitting; (ii) huge performance degradation when there is a domain shift in the data distribution between training and testing. In this paper, we investigate the problem of domain adaptation for NER under homogeneous and heterogeneous settings. We propose MetaNER, a novel meta-learning approach for domain adaptation in NER. Specifically, MetaNER incorporates meta-learning and adversarial training strategies to encourage robust, general and transferable representations for sequence labeling. The key advantage of MetaNER is that it is capable of adapting to new unseen domains with a small amount of annotated data from those domains. We extensively evaluate MetaNER on multiple datasets under homogeneous and heterogeneous settings. The experimental results show that MetaNER achieves state-of-the-art performance against eight baselines. Impressively, MetaNER surpasses the in-domain performance using only 16.17% and 34.76% of target domain data on average for homogeneous and heterogeneous settings, respectively.
   </div>

      <div id="bibtex_metaner" style="display:none">
      <pre class="verbatim">
@inproceedings{li20metaner,
author    = {Jing Li and Shuo Shang and Ling Shao},
title     = {MetaNER: Named Entity Recognition with Meta-Learning},
booktitle = {The Web Conference 2020 (WWW)},
pages     = {429--440},
year      = {2020},
url       = {https://doi.org/10.1145/3366423.3380127},
}
      </pre>
      </div>

</i> </confli>






<confli > <strong>Pay Your Trip for Traffic Congestion: Dynamic Pricing in Traffic-Aware Road Networks</strong><br/>
      <i>Lisi Chen, Shuo Shang, Bin Yao and <strong>Jing Li</strong><br/>
      <span class="abbpaper">AAAI-20</span>- The Thirty-Fourth AAAI Conference on Artificial Intelligence. Acceptance rate: 1591/7737 (20.6%). <br/>

      <a class="file_link" href="https://ojs.aaai.org//index.php/AAAI/article/view/5397"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_pay').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_pay').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_pay" class="verbatim" style="display:none">
Pricing is essential in optimizing transportation resource allocation. Congestion pricing is widely used to reduce urban traffic congestion. We propose and investigate a novel Dynamic Pricing Strategy (DPS) to price travelers' trips in intelligent transportation platforms (e.g., DiDi, Lyft, Uber). The trips are charged according to their “congestion contributions” to global urban traffic systems. The dynamic pricing strategy retrieves a matching between n travelers' trips and the potential travel routes (each trip has k potential routes) to minimize the global traffic congestion. We believe that DPS holds the potential to benefit society and the environment, such as reducing traffic congestion and enabling smarter and greener transportation. The DPS problem is challenging due to its high computation complexity (there exist kn matching possibilities). We develop an efficient and effective approximate matching algorithm based on local search, as well as pruning techniques to further enhance the matching efficiency. The accuracy and efficiency of the dynamic pricing strategy are verified by extensive experiments on real datasets.
   </div>

      <div id="bibtex_pay" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/aaai/ChenSYL20,
  author    = {Lisi Chen and
               Shuo Shang and
               Bin Yao and
               Jing Li},
  title     = {Pay Your Trip for Traffic Congestion: Dynamic Pricing in Traffic-Aware
               Road Networks},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence (AAAI)},
  pages     = {582--589},
  year      = {2020},
  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/5397},
}
      </pre>
      </div>

</i> </confli>









<!-- <p style="margin: 0rem 0 1rem -3rem;"><strong>Published</strong></p> -->

<confli > <strong>Adversarial Transfer for Named Entity Boundary Detection with Pointer Networks</strong><br/>
      <i> <strong>Jing Li</strong>, Deheng Ye and Shuo Shang<br/>
      <span class="abbpaper">IJCAI-19</span>- The 28th International Joint Conference on Artificial Intelligence, Pages 5053-5069, 2019. Acceptance rate: 850/4752 (17.9%). <br/>

      <a class="file_link" href="https://www.ijcai.org/proceedings/2019/0702.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_adt19').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_adt19').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_adt19" class="verbatim" style="display:none">
In this paper, we focus on named entity boundary detection, which aims to detect the start and end boundaries of an entity mention in text, without predicting its type. A more accurate and robust detection approach is desired to alleviate error propagation in downstream applications, such as entity linking and fine-grained typing systems. Here, we first develop a novel entity boundary labeling approach with pointer networks, where the output dictionary size depends on the input, which is variable. Furthermore, we propose AT-Bdry, which incorporates adversarial transfer learning into an end-to-end sequence labeling model to encourage domain-invariant representations. More importantly, AT-Bdry can reduce domain difference in data distributions between the source and target domains, via an unsupervised transfer learning approach (i.e., no annotated target-domain data is necessary). We conduct Formal Text to Formal Text, Formal Text to Informal Text and ablation evaluations on five benchmark datasets. Experimental results show that AT-Bdry achieves state-of-the-art transferring performance against recent baselines. 
   </div>

      <div id="bibtex_adt19" style="display:none">
      <pre class="verbatim">
@inproceedings{li19advt,
author    = {Jing Li and Deheng Ye andd Shuo Shang},
title     = {Adversarial Transfer for Named Entity Boundary Detection with Pointer Networks},
booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)},
pages     = {5053--5059},
year      = {2019},
url       = {https://doi.org/10.24963/ijcai.2019/702},
}
      </pre>
      </div>

</i> </confli>







<confli > <strong>Neural Discourse Segmentation</strong><br/>
      <i> <strong>Jing Li</strong><br/>
      <span class="abbpaper">IJCAI-19</span>- The 28th International Joint Conference on Artificial Intelligence, Pages 6539-6541, 2019.  (Demo) <br/>

      <a class="file_link" href="https://www.ijcai.org/proceedings/2019/0949.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_segdemo').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_segdemo').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_segdemo" class="verbatim" style="display:none">
Identifying discourse structures and coherence relations in a piece of text is a fundamental task in natural language processing. The first step of this process is segmenting sentences into clause-like units called elementary discourse units (EDUs). Traditional solutions to discourse segmentation heavily rely on carefully designed features. In this demonstration, we present SEGBOT, a system to split a given piece of text into sequence of EDUs by using an end-to-end neural segmentation model. Our model does not require hand-crafted features or external knowledge except word embeddings, yet it outperforms state-of-the-art solutions to discourse segmentation.
   </div>

      <div id="bibtex_segdemo" style="display:none">
      <pre class="verbatim">
@inproceedings{li19segdemo,
author    = {Jing Li},
title     = {Neural Discourse Segmentation},
booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)},
pages     = {6539--6541},
year      = {2019},
url       = {https://doi.org/10.24963/ijcai.2019/949},
}
      </pre>
      </div>

</i> </confli>










<li> <strong>LinkLive: Discovering web learning resources for developers from Q&A discussions</strong><br/>
      <i> <strong>Jing Li</strong>, Zhenchang Xing and Aixin Sun<br/>
      <span class="abbpaper">WWWJ-19</span>- World Wide Web. 22(4), Pages 1699-1725, Springer, 2019. <br/>

      <a class="file_link" href="papers/19wwwj.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_wwwj19').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_wwwj19').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_wwwj19" class="verbatim" style="display:none">
Software developers need access to correlated information (e.g., API documentation, Wikipedia pages, Stack Overflow questions and answers) which are often dispersed among different Web resources. This paper is concerned with the situation where a developer is visiting a Web page, but at the same time is willing to explore correlated Web resources to extend his/her knowledge or to satisfy his/her curiosity. Specifically, we present an item-based collaborative filtering technique, named LinkLive, for automatically recommending a list of correlated Web resources for a particular Web page. The recommendation is done by exploiting hyperlink associations from the crowdsourced knowledge on Stack Overflow. We motivate our research using an exploratory study of hyperlink dissemination patterns on Stack Overflow. We then present our LinkLive technique that uses multiple features, including hyperlink co-occurrences in Q&A discussions, locations (e.g., question, answer, or comment) in which hyperlinks are referenced, and votes for posts/comments in which hyperlinks are referenced. Experiments using 7 years of Stack Overflow data show that, our technique recommends correlated Web resources with promising accuracy in an open setting. A user study of 6 participants suggests that practitioners find the recommended Web resources useful for Web discovery.
     </div>

      <div id="bibtex_wwwj19" style="display:none">
      <pre class="verbatim">
@article{LiXS19,
  author    = {Jing Li and Zhenchang Xing and Aixin Sun},
  title     = {LinkLive: discovering Web learning resources for developers from Q{\&}A discussions},
  journal   = {World Wide Web},
  volume    = {22},
  number    = {4},
  pages     = {1699--1725},
  year      = {2019},
  url       = {https://doi.org/10.1007/s11280-018-0621-y},
  doi       = {10.1007/s11280-018-0621-y},
}
      </pre>
      </div>

</i> </li>







<confli > <strong>DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets</strong><br/>
      <i> Canwen Xu, <strong>Jing Li</strong>, Xiangyang Luo, Jiaxin Pei, Chenliang Li, Donghong Ji<br/>
      <span class="abbpaper">WWW-19</span>- The Web Conference, Pages 3391-3397, ACM, 2019. (Short) <br/>

      <a class="file_link" href="https://arxiv.org/abs/1901.07005"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_dlocrl').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_dlocrl').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_dlocrl" class="verbatim" style="display:none">
In recent years, with the prevalence of social media and smart devices, people causally reveal their locations such as shops, hotels, and restaurants in their tweets. Recognizing and linking such fine-grained location mentions to well-defined location profiles are beneficial for retrieval and recommendation systems. In this paper, we propose DLocRL, a new deep learning pipeline for fine-grained location recognition and linking in tweets, and verify its effectiveness on a real-world Twitter dataset.
   </div>

      <div id="bibtex_dlocrl" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/www/XuLLPLJ19,
  author    = {Canwen Xu and
               Jing Li and
               Xiangyang Luo and
               Jiaxin Pei and
               Chenliang Li and
               Donghong Ji},
  title     = {DLocRL: {A} Deep Learning Pipeline for Fine-Grained Location Recognition
               and Linking in Tweets},
  booktitle = {The World Wide Web Conference (WWW)},
  pages     = {3391--3397},
  year      = {2019},
  url       = {https://doi.org/10.1145/3308558.3313491},
  doi       = {10.1145/3308558.3313491},
}
      </pre>
      </div>

</i> </confli>







<li> <strong>Spatial Keyword Search: A Survey</strong><br/>
      <i> Lisi Chen, Shuo Shang, Chengcheng Yang and <strong>Jing Li</strong><br/>
      <span class="abbpaper">GeoInformatica-19</span>- GeoInformatica. Springer, July 2019.  <br/> 

      <a class="file_link" href="https://rdcu.be/bIGLV"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_spatialsurvey').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_spatialsurvey').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_spatialsurvey" class="verbatim" style="display:none">
Spatial keyword search has been playing an indispensable role in personalized route recommendation and geo-textual information retrieval. In this light, we conduct a survey on existing studies of spatial keyword search. We categorize existing works of spatial keyword search based on the types of their input data, output results, and methodologies. For each category, we summarize their common features in terms of input data, output result, indexing scheme, and search algorithms. In addition, we provide detailed description regarding each study of spatial keyword search. This survey summarizes the findings of existing spatial keyword search studies, thus uncovering new insights that may guide software engineers as well as further research.
     </div>

      <div id="bibtex_spatialsurvey" style="display:none">
      <pre class="verbatim">
@article{DBLP:journals/geoinformatica/ChenSYL20,
  author    = {Lisi Chen and
               Shuo Shang and
               Chengcheng Yang and
               Jing Li},
  title     = {Spatial keyword search: a survey},
  journal   = {GeoInformatica},
  volume    = {24},
  number    = {1},
  pages     = {85--106},
  year      = {2020},
  url       = {https://doi.org/10.1007/s10707-019-00373-y},
  doi       = {10.1007/s10707-019-00373-y},
      </pre>
      </div>

</i> </li>






<confli > <strong>Subtopic-Driven Multi-Document Summarization</strong><br/>
      <i>Xin Zheng, Aixin Sun, <strong>Jing Li</strong> and Karthik Muthuswamy <br/>
      <span class="abbpaper">EMNLP-IJCNLP-19</span>- 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Pages 3144-3153, 2019. Acceptance rate: 684/2877 (23.8%). <br/>

      <a class="file_link" href="https://aclanthology.org/D19-1311.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_xinemnlp').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_xinemnlp').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_xinemnlp" class="verbatim" style="display:none">
In multi-document summarization, a set of documents to be summarized is assumed to be on the same topic, known as the underlying topic in this paper. That is, the underlying topic can be collectively represented by all the documents in the set. Meanwhile, different documents may cover various different subtopics and the same subtopic can be across several documents. Inspired by topic model, the underlying topic of a document set can also be viewed as a collection of different subtopics of different importance. In this paper, we propose a summarization model called STDS. The model generates the underlying topic representation from both document view and subtopic view in parallel. The learning objective is to minimize the distance between the representations learned from the two views. The contextual information is encoded through a hierarchical RNN architecture. Sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience, by considering the contextual information. Top ranked sentences are then extracted as a summary. Note that the notion of subtopic enables us to bring in additional information (e.g. comments to news articles) that is helpful for document summarization. Experimental results show that the proposed solution outperforms state-of-the-art methods on benchmark datasets.
   </div>

      <div id="bibtex_xinemnlp" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/emnlp/ZhengSLM19,
  author    = {Xin Zheng and
               Aixin Sun and
               Jing Li and
               Karthik Muthuswamy},
  title     = {Subtopic-driven Multi-Document Summarization},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural
               Language Processing and the 9th International Joint Conference on
               Natural Language Processing (EMNLP-IJCNLP)},
  pages     = {3151--3160},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-1311},
  doi       = {10.18653/v1/D19-1311},
}
      </pre>
      </div>

</i> </confli>






<li > <strong>To Do or Not To Do: Distill Crowdsourced Negative Caveats to Augment API Documentation</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun and Zhenchang Xing<br/>
       <span class="abbpaper">JASIST-18</span>- Journal of the Association for Information Science and Technology. Volume 69, Issue 12, Pages 1460-1475, Wiley, 2018.<br/>

      <a class="file_link" href="papers/18jasist.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_jasist18').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_jasist18').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_jasist18" class="verbatim" style="display:none">
Negative caveats of application programming interfaces (APIs) are about “how not to use an API,” which are often absent from the official API documentation. When these caveats are overlooked, programming errors may emerge from misusing APIs, leading to heavy discussions on Q&A websites like Stack Overflow. If the overlooked caveats could be mined from these discussions, they would be beneficial for programmers to avoid misuse of APIs. However, it is challenging because the discussions are informal, redundant, and diverse. For this, for example, we propose Disca, a novel approach for automatically Distilling desirable API negative caveats from unstructured Q&A discussions. Through sentence selection and prominent term clustering, Disca ensures that distilled caveats are context‐independent, prominent, semantically diverse, and nonredundant. Quantitative evaluation in our experiments shows that the proposed Disca significantly outperforms four text‐summarization techniques. We also show that the distilled API negative caveats could greatly augment API documentation through qualitative analysis.
     </div>

      <div id="bibtex_jasist18" style="display:none">
      <pre class="verbatim">
@article{LiSX18,
  author    = {Jing Li and Aixin Sun and Zhenchang Xing},
  title     = {To Do or Not To Do: Distill crowdsourced negative caveats to augment api documentation},
  journal   = {J. Assoc. Inf. Sci. Technol.},
  volume    = {69},
  number    = {12},
  pages     = {1460--1475},
  year      = {2018},
  url       = {https://doi.org/10.1002/asi.24067},
  doi       = {10.1002/asi.24067},
}
      </pre>
      </div>

</i> </li>




<confli  > <strong>SegBot: A Generic Neural Text Segmentation Model with Pointer Network</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun and Shafiq Joty<br/>
      <span class="abbpaper">IJCAI-18</span>-The 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence. Pages 4166-4172, 2018. Acceptance rate: 710/3470 (20.5%).  <br/>

      <a class="file_link" href="https://www.ijcai.org/proceedings/2018/0579.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_segbot').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_segbot').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>  &nbsp 

       <a class="file_link" href="http://138.197.118.157:8000/segbot/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> Demo</a>  


      <br/>

      <div id="abstract_segbot" class="verbatim" style="display:none">
Text segmentation is a fundamental task in natural language processing that comes in two levels of granularity: (i) segmenting a document into a sequence of topical segments (topic segmentation), and (ii) segmenting a sentence into a sequence of elementary discourse units (EDU segmentation). Traditional solutions to the two tasks heavily rely on carefully designed features. The recently proposed neural models do not need manual feature engineering, but they either suffer from sparse boundary tags or they cannot well handle the issue of variable size output vocabulary. We propose a generic end-to-end segmentation model called SegBot. SegBot uses a bidirectional recurrent neural network to encode input text sequence. The model then uses another recurrent neural network together with a pointer network to select text boundaries in the input sequence. In this way, SegBot does not require hand-crafted features. More importantly, our model inherently handles the issue of variable size output vocabulary and the issue of sparse boundary tags. In our experiments, SegBot outperforms state-of-the-art models on both topic and EDU segmentation tasks.
   </div>

      <div id="bibtex_segbot" style="display:none">
      <pre class="verbatim">
@inproceedings{LiSJ18segbot,
  author    = {Jing Li and Aixin Sun and Shafiq R. Joty},
  title     = {SegBot: {A} Generic Neural Text Segmentation Model with Pointer Network},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)},
  pages     = {4166--4172},
  year      = {2018},
  url       = {https://doi.org/10.24963/ijcai.2018/579},
  doi       = {10.24963/ijcai.2018/579},
}
      </pre>
      </div>

</i> </confli>





<confli> <strong>API Caveat Explorer: Surfacing Nagative Usages from Practice</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun, Zhenchang Xing and Lei Han<br/>
      <span class="abbpaper">SIGIR-18</span>-The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, Pages 1293-1296. ACM, 2018. (Demo)  <br/>

      <a class="file_link" href="papers/18sigirdemo.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_apicat').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_apicat').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a>  &nbsp 

       <a class="file_link" href="http://138.197.118.157:8000/caveat/"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> Demo</a>  


      <br/>

      <div id="abstract_apicat" class="verbatim" style="display:none">
Application programming interface (API) documentation well describes an API and how to use it. However, official documentation does not describe "how not to use it" or the different kinds of errors when an API is used wrongly. Programming caveats are negative usages of an API. When these caveats are overlooked, errors may emerge, leading to heavy discussions on Q&A websites like Stack Overflow. In this demonstration, we present API Caveat Explorer, a search system to explore API caveats that are mined from large-scale unstructured discussions on Stack Overflow. API Caveat Explorer takes API-oriented queries such as "HashMap" and retrieves API caveats by text summarization techniques. API caveats are represented by sentences, which are context-independent, prominent, semantically diverse and non-redundant. The system provides a web-based interface that allows users to interactively explore the full picture of all discovered caveats of an API, and the details of each. The potential users of API Caveat Explorer are programmers and educators for learning and teaching APIs.
   </div>

      <div id="bibtex_apicat" style="display:none">
      <pre class="verbatim">
@inproceedings{LiSXH18,
  author    = {Jing Li and
               Aixin Sun and
               Zhenchang Xing and
               Lei Han},
  title     = {{API} Caveat Explorer - Surfacing Negative Usages from Practice: An
               API-oriented Interactive Exploratory Search System for Programmers},
  booktitle = {The 41st International {ACM} {SIGIR} Conference on Research {\&}
               Development in Information Retrieval},
  pages     = {1293--1296},
  year      = {2018},
  url       = {https://doi.org/10.1145/3209978.3210170},
  doi       = {10.1145/3209978.3210170},
}
      </pre>
      </div>

</i> </confli>



<li > <strong>Learning to Answer Programming Questions with Software Documentation through Social Context Embedding</strong><br/>
      <i> <strong>Jing Li</strong>, Aixin Sun and Zhenchang Xing<br/>
      <span class="abbpaper">INS-18</span>- Information Sciences. Volumes 448–449, Pages 36-52,  June 2018, Elsevier. <br/>

      <a class="file_link" href="papers/18LearingtoAnswer.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_l2a18').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_l2a18').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_l2a18" class="verbatim" style="display:none">
Official software documentation provides a comprehensive overview of software usages, but not on specific programming tasks or use cases. Often there is a mismatch between the documentation and a question on a specific programming task because of different wordings. We observe from Stack Overflow that the best answers to programmers’ questions often contain links to formal documentation. In this paper, we propose a novel deep-learning-to-answer framework, named QDLinker, for answering programming questions with software documentation. QDLinker learns from the large volume of discussions in community-based question answering site to bridge the semantic gap between programmers’ questions and software documentation. Specifically, QDLinker learns question-documentation semantic representation from these question answering discussions with a four-layer neural network, and incorporates semantic and content features into a learning-to-rank schema. Our approach does not require manual feature engineering or external resources to infer the degree of relevance between a question and documentation. Through extensive experiments, results show that QDLinker effectively answers programming questions with direct links to software documentation. QDLinker significantly outperforms the baselines based on traditional retrieval models and Web search services dedicated for software documentation retrieval. The user study shows that QDLinker effectively bridges the semantic gap between the intent of a programming question and the content of software documentation.
     </div>

      <div id="bibtex_l2a18" style="display:none">
      <pre class="verbatim">
@article{L2ALiSX18,
  author    = {Jing Li and
               Aixin Sun and
               Zhenchang Xing},
  title     = {Learning to answer programming questions with software documentation
               through social context embedding},
  journal   = {Information Sciences},
  volume    = {448-449},
  pages     = {36--52},
  year      = {2018},
  url       = {https://doi.org/10.1016/j.ins.2018.03.014},
  doi       = {10.1016/j.ins.2018.03.014},
}
      </pre>
      </div>

</i> </li>















<confli > <strong>HDSKG: Harvesting Domain Specific Knowledge Graph from Content of Webpages</strong><br/>
      <i> Xuejiao Zhao, Zhenchang Xing, Muhammad Ashad Kabir, Naoya Sawada, <strong>Jing Li</strong> and Shangwei Lin<br/>
      <span class="abbpaper">SANER-17</span>-The 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering. Acceptance rate: 34/140 (24.3%).   <br/>

      <a class="file_link" href="papers/17saner.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_hdskg').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_hdskg').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_hdskg" class="verbatim" style="display:none">
Knowledge graph is useful for many different domains like search result ranking, recommendation, exploratory search, etc. It integrates structural information of concepts across multiple information sources, and links these concepts together. The extraction of domain specific relation triples (subject, verb phrase, object) is one of the important techniques for domain specific knowledge graph construction. In this research, an automatic method named HDSKG is proposed to discover domain specific concepts and their relation triples from the content of webpages. We incorporate the dependency parser with rule-based method to chunk the relations triple candidates, then we extract advanced features of these candidate relation triples to estimate the domain relevance by a machine learning algorithm. For the evaluation of our method, we apply HDSKG to Stack Overflow (a Q&A website about computer programming). As a result, we construct a knowledge graph of software engineering domain with 35279 relation triples, 44800 concepts, and 9660 unique verb phrases. The experimental results show that both the precision and recall of HDSKG (0.78 and 0.7 respectively) is much higher than the openIE (0.11 and 0.6 respectively). The performance is particularly efficient in the case of complex sentences. Further more, with the self-training technique we used in the classifier, HDSKG can be applied to other domain easily with less training data.
   </div>

      <div id="bibtex_hdskg" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/wcre/ZhaoXKSLL17,
  author    = {Xuejiao Zhao and
               Zhenchang Xing and
               Muhammad Ashad Kabir and
               Naoya Sawada and
               Jing Li and
               Shang{-}Wei Lin},
  editor    = {Martin Pinzger and
               Gabriele Bavota and
               Andrian Marcus},
  title     = {{HDSKG:} Harvesting domain specific knowledge graph from content of
               webpages},
  booktitle = {{IEEE} 24th International Conference on Software Analysis, Evolution
               and Reengineering (SANER)},
  pages     = {56--67},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/SANER.2017.7884609},
  doi       = {10.1109/SANER.2017.7884609},
}
      </pre>
      </div>

</i> </confli>












<confli > <strong>From Discussion to Wisdom: Web Resource Recommendation for Hyperlinks in Stack Overflow</strong><br/>
      <i> <strong>Jing Li</strong>, Zhenchang Xing, Deheng Ye and Xuejiao Zhao<br/>
      <span class="abbpaper">SAC-16</span>-The 31st ACM Symposium on Applied Computing,2016. Acceptance rate: 252/1047 (24.07%).  <br/>

      <a class="file_link" href="papers/16sacwislinker.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_wislinker').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_wislinker').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_wislinker" class="verbatim" style="display:none">
Application programming interface (API) documentation well describes an API and how to use it. However, official documentation does not describe "how not to use it" or the different kinds of errors when an API is used wrongly. Programming caveats are negative usages of an API. When these caveats are overlooked, errors may emerge, leading to heavy discussions on Q&A websites like Stack Overflow. In this demonstration, we present API Caveat Explorer, a search system to explore API caveats that are mined from large-scale unstructured discussions on Stack Overflow. API Caveat Explorer takes API-oriented queries such as "HashMap" and retrieves API caveats by text summarization techniques. API caveats are represented by sentences, which are context-independent, prominent, semantically diverse and non-redundant. The system provides a web-based interface that allows users to interactively explore the full picture of all discovered caveats of an API, and the details of each. The potential users of API Caveat Explorer are programmers and educators for learning and teaching APIs.
   </div>

      <div id="bibtex_wislinker" style="display:none">
      <pre class="verbatim">
@inproceedings{SACLiXYZ16,
  author    = {Jing Li and
               Zhenchang Xing and
               Deheng Ye and
               Xuejiao Zhao},
  editor    = {Sascha Ossowski},
  title     = {From discussion to wisdom: web resource recommendation for hyperlinks
               in stack overflow},
  booktitle = {Proceedings of the 31st Annual {ACM} Symposium on Applied Computing (SAC)},
  pages     = {1127--1133},
  year      = {2016},
  url       = {https://doi.org/10.1145/2851613.2851815},
  doi       = {10.1145/2851613.2851815},
}
      </pre>
      </div>

</i> </confli>





      
<confli>   <strong>BPMiner: Mining Developers' Behavior Patterns from Screen-Captured Task Videos</strong><br/>
      <i> <strong>Jing Li</strong>, Lingfeng Bao, Zhenchang Xing, Xinyu Wang and Bo Zhou<br/>
      <span class="abbpaper">SAC-16</span>-The 31st ACM Symposium on Applied Computing, 2016. Acceptance rate: 252/1047 (24.07%).  <br/>

      <a class="file_link" href="papers/16sacbpminer.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_bpminer').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_bpminer').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_bpminer" class="verbatim" style="display:none">
Many user studies of software development use screen-capture software to record developers' behavior for post-mortem analysis. However, extracting behavioral patterns from screencaptured videos requires manual transcription and coding of videos, which is often tedious and error-prone. Automatically extracting Human-Computer Interaction (HCI) data from screen-captured videos and systematically analyzing behavioral data will help researchers analyze developers' behavior in software development more effectively and efficiently. In this paper, we present BPMiner, a novel behavior analysis approach to mine developers' behavior patterns from screencaptured videos using computer vision techniques and exploratory sequential pattern analysis. We have implemented a proof-of-concept prototype of BPMiner, and applied the BPMiner prototype to study the developers' online search behavior during software development. Our study suggests that the BPMiner approach can open up new ways to study developers' behavior in software development.
   </div>

      <div id="bibtex_bpminer" style="display:none">
      <pre class="verbatim">
@inproceedings{SACLiBXWZ16,
  author    = {Jing Li and
               Lingfeng Bao and
               Zhenchang Xing and
               Xinyu Wang and
               Bo Zhou},
  editor    = {Sascha Ossowski},
  title     = {BPMiner: mining developers' behavior patterns from screen-captured
               task videos},
  booktitle = {Proceedings of the 31st Annual {ACM} Symposium on Applied Computing (SAC)},
  pages     = {1371--1377},
  year      = {2016},
  url       = {https://doi.org/10.1145/2851613.2851771},
  doi       = {10.1145/2851613.2851771},
}
      </pre>
      </div>

</i> </confli>






<confli > <strong>Software-specific Part-of-speech Tagging: An Experimental Study on Stack Overflow</strong><br/>
      <i> Deheng Ye, Zhenchang Xing, <strong>Jing Li</strong> and Nachiket Kapre<br/>
      <span class="abbpaper">SAC-16</span>-The 31st ACM Symposium on Applied Computing, 2016. Acceptance rate: 252/1047 (24.07%). <br/>

      <a class="file_link" href="papers/16sacpos.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_pos').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_pos').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_pos" class="verbatim" style="display:none">
Part-of-speech (POS) tagging performance degrades on out-of-domain data due to the lack of domain knowledge. Software engineering knowledge, embodied in textual documentations, bug reports and online forum discussions, is expressed in natural language, but is full of domain terms, software entities and software-specific informal languages. Such software texts call for software-specific POS tagging. In the software engineering community, there have been several attempts leveraging POS tagging technique to help solve software engineering tasks. However, little work is done for POS tagging on software natural language texts. In this paper, we build a software-specific POS tagger, called S-POS, for processing the textual discussions on Stack Overflow. We target at Stack Overflow because it has become an important developer-generated knowledge repository for software engineering. We define a POS tagset that is suitable for describing software engineering knowledge, select corpus, develop a custom tokenizer, annotate data, design features for supervised model training, and demonstrate that the tagging accuracy of S-POS outperforms that of the Stanford POS Tagger when tagging software texts. Our work presents a feasible roadmap to build software-specific POS tagger for the socio-professional contents on Stack Overflow, and reveals challenges and opportunities for advanced software-specific information extraction.
   </div>

      <div id="bibtex_pos" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/sac/YeXLK16,
  author    = {Deheng Ye and
               Zhenchang Xing and
               Jing Li and
               Nachiket Kapre},
  editor    = {Sascha Ossowski},
  title     = {Software-specific part-of-speech tagging: an experimental study on
               stack overflow},
  booktitle = {Proceedings of the 31st Annual {ACM} Symposium on Applied Computing (SAC)},
  pages     = {1378--1385},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2851613.2851772},
  doi       = {10.1145/2851613.2851772},
}
      </pre>
      </div>

</i> </confli>






<li  > <strong>Extracting and Analyzing Time-Series HCI Data from Screen-Captured Task Videos</strong><br/>
      <i> Lingfeng Bao, <strong>Jing Li</strong>, Zhenchang Xing, Xinyu Wang, Xin xia and Bo Zhou<br/>
      <span class="abbpaper">EMSE-16</span>- Empirical Software Engineering, Springer, Pages 1-41, 2016. <br/>

      <a class="file_link" href="papers/16emse.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_emse').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_emse').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_emse" class="verbatim" style="display:none">
Recent years have witnessed the increasing emphasis on human aspects in software engineering research and practices. Our survey of existing studies on human aspects in software engineering shows that screen-captured videos have been widely used to record developers’ behavior and study software engineering practices. The screen-captured videos provide direct information about which software tools the developers interact with and which content they access or generate during the task. Such Human-Computer Interaction (HCI) data can help researchers and practitioners understand and improve software engineering practices from human perspective. However, extracting time-series HCI data from screen-captured task videos requires manual transcribing and coding of videos, which is tedious and error-prone. In this paper we report a formative study to understand the challenges in manually transcribing screen-captured videos into time-series HCI data. We then present a computer-vision based video scraping technique to automatically extract time-series HCI data from screen-captured videos. We also present a case study of our scvRipper tool that implements the video scraping technique using 29-hours of task videos of 20 developers in two development tasks. The case study not only evaluates the runtime performance and robustness of the tool, but also performs a detailed quantitative analysis of the tool’s ability to extract time-series HCI data from screen-captured task videos. We also study the developer’s micro-level behavior patterns in software development from the quantitative analysis.
     </div>

      <div id="bibtex_emse" style="display:none">
      <pre class="verbatim">
@article{DBLP:journals/ese/BaoLXWXZ17,
  author    = {Lingfeng Bao and
               Jing Li and
               Zhenchang Xing and
               Xinyu Wang and
               Xin Xia and
               Bo Zhou},
  title     = {Extracting and analyzing time-series {HCI} data from screen-captured
               task videos},
  journal   = {Empir. Softw. Eng.},
  volume    = {22},
  number    = {1},
  pages     = {134--174},
  year      = {2017},
  url       = {https://doi.org/10.1007/s10664-015-9417-1},
  doi       = {10.1007/s10664-015-9417-1},
}
      </pre>
      </div>

</i> </li>





<confli > <strong>Learning to Extract API Mentions from Informal Natural Language Discussions</strong><br/>
      <i> Deheng Ye, Zhenchang Xing, Chee Yong Foo, <strong>Jing Li</strong>, and Nachiket Kapre<br/>
      <span class="abbpaper">ICSME-16</span>-The 32nd International Conference on Software Maintenance and Evolution. Acceptance rate: 37/125 (29%).  <br/>

      <a class="file_link" href="papers/16icsme.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_icsme').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_icsme').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_icsme" class="verbatim" style="display:none">
When discussing programming issues on social platforms (e.g, Stack Overflow, Twitter), developers often mention APIs in natural language texts. Extracting API mentions in natural language texts is a prerequisite for effective indexing and searching for API-related information in software engineering social content. However, the informal nature of social discussions creates two fundamental challenges for API extraction: common-word polysemy and sentence-format variations. Common-word polysemy refers to the ambiguity between the API sense of a common word and the normal sense of the word (e.g., append, apply and merge). Sentence-format variations refer to the lack of consistent sentence writing format for inferring API mentions. Existing API extraction techniques fall short to address these two challenges, because they assume distinct API naming conventions (e.g., camel case, underscore) or structured sentence format (e.g., code-like phrase, API annotation, or full API name). In this paper, we propose a semi-supervised machine-learning approach that exploits name synonyms and rich semantic context of API mentions to extract API mentions in informal social text. The key innovation of our approach is to exploit two complementary unsupervised language models learned from the abundant unlabeled text to model sentence-format variations and to train a robust model with a small set of labeled data and an iterative self-training process. The evaluation of 1,205 API mentions of the three libraries (Pandas, Numpy, and Matplotlib) in Stack Overflow texts shows that our approach significantly outperforms existing API extraction techniques based on language-convention and sentence-format heuristics and our earlier machine-learning based method for named-entity recognition.
   </div>

      <div id="bibtex_icsme" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/icsm/YeXFLK16,
  author    = {Deheng Ye and
               Zhenchang Xing and
               Chee Yong Foo and
               Jing Li and
               Nachiket Kapre},
  title     = {Learning to Extract {API} Mentions from Informal Natural Language
               Discussions},
  booktitle = {2016 {IEEE} International Conference on Software Maintenance and Evolution (ICSME)},
  pages     = {389--399},
  year      = {2016},
  url       = {https://doi.org/10.1109/ICSME.2016.11},
  doi       = {10.1109/ICSME.2016.11},
}
      </pre>
      </div>

</i> </confli>







<confli > <strong>Software-specific Named Entity Recognition in Software Engineering Social Content</strong><br/>
      <i> Deheng Ye, Zhenchang Xing, Chee Yong Foo, Zi Qun Ang, <strong>Jing Li</strong> and Nachiket Kapre<br/>
      <span class="abbpaper">SANER-16</span>-The 23rd IEEE International Conference on Software Analysis, Evolution, and Reengineering. Acceptance rate: 52/140 (37%).   <br/>

      <a class="file_link" href="papers/16saner.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_sner').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_sner').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_sner" class="verbatim" style="display:none">
Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.
   </div>

      <div id="bibtex_sner" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/wcre/YeXFALK16,
  author    = {Deheng Ye and
               Zhenchang Xing and
               Chee Yong Foo and
               Zi Qun Ang and
               Jing Li and
               Nachiket Kapre},
  title     = {Software-Specific Named Entity Recognition in Software Engineering
               Social Content},
  booktitle = {{IEEE} 23rd International Conference on Software Analysis, Evolution,
               and Reengineering (SANER)},
  pages     = {90--101},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/SANER.2016.10},
  doi       = {10.1109/SANER.2016.10},
}
      </pre>
      </div>

</i> </confli>







<confli > <strong>scvRipper: Video Scraping Tool for Modeling Developers' Behavior Using Interaction Data</strong><br/>
      <i> Lingfeng Bao, <strong>Jing Li</strong>, Zhenchang Xing, Xinyu Wang and Bo Zhou <br/>
      <span class="abbpaper">ICSE-15</span>-The 37th International Conference on Software Engineering Tool Demonstrations, Vol.2, Pages 673-676, 2015.   <br/>

      <a class="file_link" href="papers/15icse.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_scvripper').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 


      <a class="file_link" href="https://www.youtube.com/watch?v=DElYOhids8Y"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> Demo</a>   &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_scvripper').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_scvripper" class="verbatim" style="display:none">
Screen-capture tool can record a user's interaction with software and application content as a stream of screenshots which is usually stored in certain video format. Researchers have used screen-captured videos to study the programming activities that the developers carry out. In these studies, screen-captured videos had to be manually transcribed to extract software usage and application content data for the study purpose. This paper presents a computer-vision based video scraping tool (called scvRipper) that can automatically transcribe a screen-captured video into time-series interaction data according to the analyst's need. This tool can address the increasing need for automatic behavioral data collection methods in the studies of human aspects of software engineering.
   </div>

      <div id="bibtex_scvripper" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/icse/BaoLXWZ15,
  author    = {Lingfeng Bao and
               Jing Li and
               Zhenchang Xing and
               Xinyu Wang and
               Bo Zhou},
  title     = {scvRipper: Video Scraping Tool for Modeling Developers' Behavior Using
               Interaction Data},
  booktitle = {37th {IEEE/ACM} International Conference on Software Engineering,
               {ICSE} 2015, Florence, Italy, May 16-24, 2015, Volume 2},
  pages     = {673--676},
  publisher = {{IEEE} Computer Society},
  year      = {2015},
  url       = {https://doi.org/10.1109/ICSE.2015.220},
  doi       = {10.1109/ICSE.2015.220},
}    </pre>
      </div>

</i> </confli>










<confli > <strong>Reverse Engineering Time-Series Interaction Data from Screen-Captured Videos</strong><br/>
      <i> Lingfeng Bao, <strong>Jing Li</strong>, Zhenchang Xing, Xinyu Wang and Bo Zhou <br/>
      <span class="abbpaper">SANER-15</span>-The 22nd IEEE International Conference on Software Analysis, Evolution, and Reengineering, Pages 399-408, 2015. Acceptance rate: 46/144 (32%).  <br/>

      <a class="file_link" href="papers/15saner.pdf"  target="_blank" style="text-decoration:none;"> <i class="fa fa-file-pdf-o"></i> PDF</a>  &nbsp 


      <a class="file_link" data-toggle="dropdown" href="javascript:void(0);" style="text-decoration:none;" onclick="$('#abstract_s15').slideToggle('fast');return false;">
                             <i class="fa fa-file-text-o"></i> Abstract</a> &nbsp 

      <a class="file_link" data-toggle="dropdown" style="text-decoration:none;" href="javascript:void(0);" onclick="$('#bibtex_s15').slideToggle('fast');return false;"> <i class="fa fa-book"></i> BibTex</a> 


      <br/>

      <div id="abstract_s15" class="verbatim" style="display:none">
In recent years the amount of research on human aspects of software engineering has increased. Many studies use screen-capture software (e.g., Snagit) to record developers' behavior as they work on software development tasks. The recorded task videos capture direct information about which activities the developers carry out with which content and in which applications during the task. Such behavioral data can help researchers and practitioners understand and improve software engineering practices from human perspective. However, extracting time-series interaction data (software usage and application content) from screen-captured videos requires manual transcribing and coding of videos, which is tedious and error-prone. In this paper we present a computer-vision based video scraping technique to automatically reverse-engineer time-series interaction data from screen-captured videos. We report the usefulness, effectiveness and runtime performance of our video scraping technique using a case study of the 29 hours task videos of 20 developers in the two development tasks.
   </div>

      <div id="bibtex_s15" style="display:none">
      <pre class="verbatim">
@inproceedings{DBLP:conf/wcre/BaoLXWZ15,
  author    = {Lingfeng Bao and
               Jing Li and
               Zhenchang Xing and
               Xinyu Wang and
               Bo Zhou},
  editor    = {Yann{-}Ga{\"{e}}l Gu{\'{e}}h{\'{e}}neuc and
               Bram Adams and
               Alexander Serebrenik},
  title     = {Reverse engineering time-series interaction data from screen-captured
               videos},
  booktitle = {22nd {IEEE} International Conference on Software Analysis, Evolution,
               and Reengineering (SANER)},
  pages     = {399--408},
  year      = {2015},
  url       = {https://doi.org/10.1109/SANER.2015.7081850},
  doi       = {10.1109/SANER.2015.7081850},
} 
</pre>
      </div>

</i> </confli>



 

<!-- </i></confli> -->



<!-- <strong>Before PhD </strong> -->

<!-- <h4 class="year">Before 2015</h4>

<br>
   <li  ><strong> Applications of Compressed Sensing for Multiple Transmitters Multiple Azimuth Beams SAR Imaging </strong><br/>
<i> <strong>Jing Li</strong>, Shunsheng Zhang,Junfei Chang <br/>
 <span class="abbpaper">PIER-12</span>-  Progress In Electromagnetics Research, Vol. 127, Pages 259–275, 2012. (<strong><span class="pierif"></span></strong>)
<a class="file_link"  href="papers/Applications of Compressed Sensing for Multiple Transmitters Multiple Azimuth Beams SAR Imaging.pdf" target="_blank">PDF </a></i></li>
     

   <li  ><strong> Forward-Looking Bistatic SAR Imaging Based On High Order Range Equation And High Order Phase Compensation  </strong><br/>
<i> Shunsheng Zhang and <strong>Jing Li*</strong><br/>
<span class="abbpaper">JEWA-12</span>- Journal of Electromagnetic Waves and Applications, vol.26, nos.17-18, pages 2304-2314, 2012. (<strong><span class="jewaif"></span></strong>)
<a class="file_link" href="papers/Forward-Looking BistaticSAR Imaging Based On High Order Range Equation And High Order Phase Compensation.pdf" target="_blank">PDF</a> *Corresponding author</i></li> 
 -->

            </confli>

      </jli>
    </div>









<!-- Journal Edition Associate Editor:Co-Guest Editor
Conference Organization Committee
Conference Area Chairs
Conference Senior PC Members
Workshop Organization
Conference PC Members
Journal Reviewers -->











</div>
<footer id="credits" >   
        <p align="center"><span class="nobreak">Copyright &copy; Jing LI &nbsp;<i class="fa fa-envelope fa-1x"/></i> lijing@li-jing.com</span></p>
    </footer>



        <div align="center">  
          <a target="_blank" href="http://www.clustrmaps.com/map/li-jing.com" title="Visitor Map for li-jing.com"><img src="//www.clustrmaps.com/map_v2.png?u=eb3a&d=9gYcBi7vGfkRh_ZDs1qVy_FrbDiLr3V4NFRNgJpr6P4" style="display: none;"/></a>
       </div>
 
 

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  </div>






</div>

</body>
</html>

